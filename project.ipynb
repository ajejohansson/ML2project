{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, random_split\n",
    "from torch.distributions import Categorical\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import shutil\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification, AutoModel, logging\n",
    "logging.set_verbosity_error()\n",
    "import tqdm\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import entropy, pearsonr\n",
    "from scipy.special import kl_div\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    '''\n",
    "    Sets seed for random, np, torch\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globals():\n",
    "    '''\n",
    "    Returns the following parameters to be set globally:\n",
    "        device, model_specifier, classes, alt_classnames, tokenizer, hyperparams, toy_run\n",
    "    '''\n",
    "    # To avoid having to pass the same variables over and over, I set some global ones here. Keeps them in the same place.\n",
    "    # Note that this means the functions will refer to values that are undefined as of function definition that will be defined\n",
    "    # before function call.\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    #args = parser.parse_args(args=[])\n",
    "    parser.add_argument(\"-c\", \"--config\", help=\"configuration file\", default=\"config.json\")\n",
    "    args = parser.parse_args(args=[])\n",
    "    config = json.load(open(args.config))\n",
    "\n",
    "    set_seed(config[\"seed\"])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(config[\"device\"])\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    model_selection = {\n",
    "    'bert_base': {\n",
    "        \"model_name\": \"bert-base-uncased\",\n",
    "        \"tokenizer\": BertTokenizer,\n",
    "        \"sequence_classification\": BertForSequenceClassification,\n",
    "    },\n",
    "    \n",
    "    'bert_large': {\n",
    "        \"model_name\": \"bert-large-uncased\",\n",
    "        \"tokenizer\": BertTokenizer,\n",
    "        \"sequence_classification\": BertForSequenceClassification,\n",
    "    },\n",
    "    'roberta_large': {\n",
    "        \"model_name\": \"roberta-large\",\n",
    "        \"tokenizer\": RobertaTokenizer,\n",
    "        \"sequence_classification\": RobertaForSequenceClassification,\n",
    "    }}\n",
    "\n",
    "    model_specifier = model_selection[config[\"model_id\"]]\n",
    "    classes = {'entailment': torch.as_tensor(0), 'neutral': torch.as_tensor(1), 'contradiction': torch.as_tensor(2)}\n",
    "    alt_classnames = {'e': 'entailment', 'n': 'neutral', 'c': 'contradiction' }\n",
    "    tokenizer = model_specifier['tokenizer'].from_pretrained(model_specifier['model_name'])\n",
    "    toy_run = config['toy_run']\n",
    "    train_eval_mode = config['train_eval_mode']\n",
    "    manual_modelname = config[\"manual_modelname\"]\n",
    "    overwrite = config[\"overwrite\"]\n",
    "    if config['manual_hyperparams']['use']:\n",
    "        hyperparams = config['manual_hyperparams']\n",
    "    elif toy_run:\n",
    "        hyperparams = {\n",
    "        'batch_size': 4,\n",
    "        'epochs': 2,\n",
    "        'lr' : 0.01\n",
    "        }\n",
    "    else:\n",
    "        hyperparams = {\n",
    "        'batch_size': 32,\n",
    "        'epochs': 6,\n",
    "        'lr' : 5e-5\n",
    "        }\n",
    "        \n",
    "\n",
    "    return device, model_specifier, classes, alt_classnames, tokenizer, hyperparams, toy_run, train_eval_mode, manual_modelname, overwrite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to get NLI variation:\n",
    "evaluation will assume this exists \n",
    "\n",
    "set to markdown since repository has this already\n",
    "\n",
    "nli_var_url = \"https://raw.githubusercontent.com/epavlick/NLI-variation-data/refs/heads/master/sentence-pair-analysis/preprocessed-data.jsonl\"\n",
    "nli_var_r = requests.get(nli_var_url)\n",
    "if not os.path.exists(\"./data/NLI_variation\"):\n",
    "        os.mkdir(\"./data/NLI_variation\")\n",
    "if not os.path.exists(\"./data/NLI_variation/NLI_variation_data.jsonl\"):\n",
    "    with open(\"./data/NLI_variation/NLI_variation_data.jsonl\", \"wb\") as f:\n",
    "        f.write(nli_var_r.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_headers(df):\n",
    "    '''\n",
    "    Normalises the column names that the different datasets come with.\n",
    "\n",
    "    Arg:\n",
    "        pandas dataframe based on any dataset used in this project\n",
    "    Returns:\n",
    "        Column-normalised dataframe\n",
    "    '''\n",
    "\n",
    "    #https://stackoverflow.com/questions/55715572/renaming-column-in-pandas-dataframe-if-condition-is-met, modified\n",
    "    \n",
    "    # there are potential data variations for which the following would be semantically incorrect, but it works for my data\n",
    "    df = df\n",
    "    alt_column_names = {'premise': ['context', 'sentence1'], 'hypothesis': ['statement', 'sentence2'], 'label': ['gold_label', 'labels', 'majority_label']}\n",
    "    df.columns = ['premise' if any(k == x for k in alt_column_names['premise']) else x for x in df]\n",
    "    df.columns = ['hypothesis' if any(k == x for k in alt_column_names['hypothesis']) else x for x in df]\n",
    "    df.columns = ['label' if any(k == x for k in alt_column_names['label']) else x for x in df]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_dict(setname):\n",
    "    '''\n",
    "    Converts raw dataset files into pandas dataframes\n",
    "    Arg: simple setname string, e.g., 'anli' for Adversarial NLI\n",
    "    Returns: dict of dataframes for the input dataset\n",
    "    '''\n",
    "    df_dict = {}\n",
    "    dirpath = './data/' + setname\n",
    "    walking = os.walk(dirpath)\n",
    "\n",
    "    for root, dirs, files in walking:\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            name, extension = os.path.splitext(full_path)\n",
    "            if extension == '.jsonl':\n",
    "                df = pd.read_json(path_or_buf=full_path, lines=True)\n",
    "                if setname == 'chaosNLI':\n",
    "                    cnli_examples = pd.json_normalize(df['example']) # p, h, label are in a single column. This makes a df with that column unpacked\n",
    "                    cnli_examples = cnli_examples.drop(['uid'], axis=1) # uid is in now redundantly in both the unpacked 'example' column and in the original df\n",
    "                    df = df.drop(['example'], axis=1)\n",
    "                    df = df.join(cnli_examples)\n",
    "            elif extension == '.csv':\n",
    "                df = pd.read_csv(filepath_or_buffer=full_path)\n",
    "            else:\n",
    "                continue # skips e.g. text files\n",
    "\n",
    "            df = normalise_headers(df)\n",
    "            \n",
    "            #anli has a one-deeper filesystem (per-round datasets), so the returned dict is also deeper\n",
    "            if setname == 'anli':\n",
    "                round, subset = name.split('/')[-2:]\n",
    "                if round not in df_dict:\n",
    "                    df_dict[round] = {}\n",
    "                if subset not in df_dict[round]:\n",
    "                    df_dict[round][subset] = df\n",
    "            else:\n",
    "                subset = name.split('/')[-1]\n",
    "                df_dict[subset] = df\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_encoded_input(tokenizer, df, max_length=None):\n",
    "    '''\n",
    "    Tokenizes premise and hypothesis (including special tokens).\n",
    "    Adds input ids, token type ids (token types mark belonging to premise or hypothesis), and attention mask to dataframe.\n",
    "    Args:\n",
    "        tokenizer\n",
    "        df to modify\n",
    "        max_length: max allowed sequence length\n",
    "    '''\n",
    "    # do not actually use max_length and there is no way to change this in the config, but it is something that could be wanted so I left it in\n",
    "\n",
    "    if 'input_encoding' in df.columns:\n",
    "        # not necessary if notebook is run like a script, but this safeguards against some notebook-type sequencing errors\n",
    "        # where an already encoding-modified df is input.\n",
    "        return df\n",
    "    \n",
    "    tokenizer = tokenizer\n",
    "    new_df = df\n",
    "    premises = list(df['premise'])\n",
    "    hypotheses = list(df['hypothesis'])\n",
    "    \n",
    "    encoded_pairs = [tokenizer.encode_plus(p, h, max_length=max_length, return_token_type_ids=True, truncation=True)\n",
    "                       for p, h in zip(premises, hypotheses)]\n",
    "    tensor_pairs = [{'input_ids': torch.as_tensor(encoded_pair['input_ids']),\n",
    "                      'token_type_ids': torch.as_tensor(encoded_pair['token_type_ids']),\n",
    "                      'attention_mask': torch.as_tensor(encoded_pair['attention_mask'])} \n",
    "                      for encoded_pair in encoded_pairs]\n",
    "    new_df.insert(0, 'input_encoding', tensor_pairs)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_encoded_labels(df):\n",
    "    '''\n",
    "    Adds tensor version of labels to dataframe\n",
    "    '''    \n",
    "    if 'label_tensor' in df.columns:\n",
    "        return df\n",
    "    new_df = df\n",
    "    encoded_labels = [classes[lb] if lb in classes else classes[alt_classnames[lb]] for lb in list(df.label)]\n",
    "    encoded_labels = torch.as_tensor(encoded_labels)\n",
    "    \n",
    "    new_df.insert(1, 'label_tensor', encoded_labels)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIData(Dataset):\n",
    "    '''\n",
    "    Subclass of pytorch Dataset\n",
    "    '''\n",
    "    def __init__(self, dataframe, require_label=True):\n",
    "\n",
    "        '''\n",
    "        args:\n",
    "            dataframe: pandas dataframe in style of get_df_dict output\n",
    "            require_label: bool indicating whether dataset has a discrete gold label\n",
    "        '''\n",
    "        # NLI variation does not have a discrete label, instead using a label distribution for each example\n",
    "\n",
    "        df = add_encoded_input(tokenizer, dataframe)\n",
    "        general_data = ['input_encoding', 'premise', 'hypothesis']\n",
    "        if require_label:\n",
    "            df = add_encoded_labels(df)\n",
    "            general_data = ['input_encoding', 'label_tensor', 'premise', 'hypothesis', 'label']\n",
    "        self.set_specific_data = df.drop(general_data, axis=1)\n",
    "        # set_specific_data is a bit of a garbage dump category that serves the purpose of making this class able to handle the diverse\n",
    "        # categories in the input sets\n",
    "\n",
    "        self.df = df\n",
    "        self.require_label = require_label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        out_dict = {'input_encoding': row['input_encoding'],\n",
    "                    'premise': row['premise'],\n",
    "                    'hypothesis': row['hypothesis'],\n",
    "                    'input_length': len(row['input_encoding']['input_ids']),\n",
    "                    'unique_data': self.set_specific_data.iloc[idx]\n",
    "                    }\n",
    "        if self.require_label:\n",
    "            out_dict['label'] = row['label']\n",
    "            out_dict['label_tensor'] = [row['label_tensor']]\n",
    "\n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_data):\n",
    "    \"\"\"\n",
    "    Collate function for pytorch dataloader\n",
    "    \"\"\"\n",
    "    # mainly used for padding\n",
    "\n",
    "    input_ids = [example['input_encoding']['input_ids'] for example in batch_data]\n",
    "    token_type_ids = [example['input_encoding']['token_type_ids'] for example in batch_data]\n",
    "    attention_masks = [example['input_encoding']['attention_mask'] for example in batch_data]\n",
    "\n",
    "    p_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    p_token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0)\n",
    "    p_attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    unique_data = [example['unique_data'] for example in batch_data]\n",
    "   \n",
    "    batch = {'input_ids': p_input_ids, 'token_type_ids': p_token_type_ids, 'attention_masks': p_attention_masks,\n",
    "             'unique_data': unique_data}\n",
    "    \n",
    "    if 'label_tensor' in batch_data[0]:\n",
    "        label_tensors = [example['label_tensor'] for example in batch_data]\n",
    "        label_tensors = torch.as_tensor(label_tensors)  #'label_tensors': label_tensors\n",
    "        batch['label_tensors'] = label_tensors\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    '''\n",
    "    Gets and cleans datasets. Dataset selection is specified by notebook config modes\n",
    "\n",
    "    Returns:\n",
    "        datasets: list of datasets. If dataset is split into subsets, those subsets are grouped in dicts.\n",
    "    '''\n",
    "\n",
    "    sets_to_get = []\n",
    "    if toy_run:\n",
    "        sets_to_get.append('evalsets') # some evals used for training in this case, since this assumes the big training sets are not locally available\n",
    "    else:\n",
    "        if not train_eval_mode == 'eval':\n",
    "            sets_to_get.append('trainsets') # trainsets = snli, mnli, anli (all rounds)\n",
    "        if not train_eval_mode == 'train':\n",
    "            sets_to_get.append('evalsets') # evalsets = cnli (both subsets), NLIvar\n",
    "    \n",
    "    # Notes:\n",
    "    # NLIvar df will always be fetched (but its dataset not always returned) since it is used to filter trainsets for duplicates\n",
    "    # Code could be restructured to require fewer identical conditionals. The reason for the structure is only that I wrote the fetching modes\n",
    "    # after the fact\n",
    "    \n",
    "    def get_NLIvar_duplicate_indices(trainset_str='snli'):\n",
    "        '''\n",
    "        Get the indices in NLI variation that correspond to examples that exist in the training data.\n",
    "        Arg:\n",
    "            trainset_str: string corresponding to a trainset\n",
    "                valid options: 'snli', 'mnli'\n",
    "        '''\n",
    "\n",
    "\n",
    "        # The NLI variation dataset partially uses inference pairs from SNLI and MNLI training data.\n",
    "        # I remove duplicates from the training sets since the effect on the size is negligible (~100 pairs\n",
    "        # per set, out of hundreds of thousands). Doing it the other way around (remove from NLIvar) would\n",
    "        # remove a notable chunk out of this eval set\n",
    "\n",
    "        # chaosNLI also draws from MNLI and SNLI, but from the dev sets, so there is no data contamination\n",
    "\n",
    "        if trainset_str == 'snli':\n",
    "            trainset = snli['snli_train']\n",
    "            trainset_id_list = list(snli['snli_train']['pairID'])\n",
    "        elif trainset_str == 'mnli':\n",
    "            trainset = mnli['multinli_train']\n",
    "            trainset_id_list = list(mnli['multinli_train']['pairID'])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        nli_var_filtered = nli_var.loc[nli_var['task'] == trainset_str]\n",
    "        nli_var_ids = list(nli_var_filtered['id'])\n",
    "        count = 0\n",
    "        duplicate_var_indices = []\n",
    "        duplicate_train_indices = []\n",
    "        for item in nli_var_ids:\n",
    "            if item in trainset_id_list:\n",
    "                count +=1\n",
    "                varidx = nli_var_filtered.loc[nli_var_filtered['id'] == item].index[0]\n",
    "                trainsetidx = trainset['pairID'].loc[trainset['pairID'] == item].index[0]\n",
    "                duplicate_var_indices.append(varidx)\n",
    "                duplicate_train_indices.append(trainsetidx)\n",
    "\n",
    "        return duplicate_train_indices\n",
    "    \n",
    "    def column_filter():\n",
    "        '''\n",
    "        Removes unused dataset columns\n",
    "        '''\n",
    "\n",
    "        if 'trainsets' in sets_to_get:\n",
    "            snli['snli_train'].drop(['annotator_labels', 'captionID', 'pairID', 'sentence1_binary_parse', 'sentence1_parse',\n",
    "                                    'sentence2_binary_parse', 'sentence2_parse' ], axis=1, inplace=True),\n",
    "            mnli['multinli_train'].drop(['annotator_labels', 'genre', 'pairID', 'promptID', 'sentence1_binary_parse', \n",
    "                                        'sentence1_parse', 'sentence2_binary_parse', 'sentence2_parse'], axis=1, inplace=True)\n",
    "            \n",
    "            for round_str, sets in anli1.items():\n",
    "                anli1[round_str]['train'].drop(['uid', 'model_label', 'emturk', 'genre', 'reason', 'tag'], axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "        if 'evalsets' in sets_to_get:\n",
    "            cnli['chaosNLI_snli'].drop(['uid', 'label_dist', 'old_labels', 'old_label', 'source'], axis=1, inplace=True),\n",
    "            cnli['chaosNLI_mnli_m'].drop(['uid', 'label_dist', 'old_labels', 'old_label', 'source'], axis=1, inplace=True),\n",
    "\n",
    "            nli_var.drop(['task', 'original-dataset-label', 'id', 'num-NA'], axis=1, inplace=True)\n",
    "    \n",
    "    #anli2 = get_df_dict('anli_reanalyzed')\n",
    "\n",
    "    if 'trainsets' in sets_to_get:\n",
    "        print('Getting SNLI dataframe...')\n",
    "        snli = get_df_dict('snli')\n",
    "        snli['snli_train'].drop(snli['snli_train'].loc[snli['snli_train']['label']=='-'].index, inplace=True) # some nonlabels in snli\n",
    "\n",
    "        print('Getting MNLI dataframe...')\n",
    "        mnli = get_df_dict('multinli')\n",
    "\n",
    "        print('Getting ANLI dataframes...')\n",
    "        anli1 = get_df_dict('anli')\n",
    "\n",
    "    \n",
    "    if 'evalsets' in sets_to_get:\n",
    "        print('Getting CNLI dataframes...')\n",
    "        cnli = get_df_dict('chaosNLI')\n",
    "    \n",
    "    print('Getting NLI variation dataframe...')\n",
    "    nli_var = pd.read_json(path_or_buf='./data/NLI_variation/NLI_variation_data.jsonl', lines=True)\n",
    "\n",
    "    if 'trainsets' in sets_to_get:\n",
    "        duplicate_snli_indices = get_NLIvar_duplicate_indices('snli')\n",
    "        duplicate_mnli_indices = get_NLIvar_duplicate_indices('mnli')\n",
    "        snli['snli_train'].drop(index=duplicate_snli_indices, inplace=True)\n",
    "        snli['snli_train'].reset_index(drop=True, inplace=True)\n",
    "        mnli['multinli_train'].drop(index=duplicate_mnli_indices, inplace=True)\n",
    "        mnli['multinli_train'].reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    column_filter()\n",
    "\n",
    "    datasets = []\n",
    "    \n",
    "    if 'trainsets' in sets_to_get:\n",
    "        print('Transforming SNLI data...')\n",
    "        snli_train_dataset = NLIData(snli['snli_train'])\n",
    "        datasets.append(snli_train_dataset)\n",
    "        \n",
    "        print('Transforming MNLI data...')\n",
    "        mnli_train_dataset = NLIData(mnli['multinli_train'])\n",
    "        datasets.append(mnli_train_dataset)\n",
    "\n",
    "\n",
    "        print('Transforming ANLI data...')\n",
    "    \n",
    "        anli_training_datasets = {}\n",
    "        for round_str in anli1.keys():\n",
    "            print('\\t', round_str+'...')\n",
    "            anli_training_datasets[round_str] = NLIData(anli1[round_str]['train'])\n",
    "        datasets.append(anli_training_datasets)\n",
    "    \n",
    "\n",
    "    if 'evalsets' in sets_to_get:\n",
    "        print('Transforming CNLI data...')\n",
    "\n",
    "        cnli_eval_datasets = {}\n",
    "        for name, df in cnli.items():\n",
    "            print('\\t', name+'...')\n",
    "            cnli_eval_datasets[name] = NLIData(df)\n",
    "        datasets.append(cnli_eval_datasets)\n",
    "\n",
    "        print('Transforming NLI variation data...')\n",
    "\n",
    "        NLI_var_eval_dataset = NLIData(nli_var, require_label=False)\n",
    "        datasets.append(NLI_var_eval_dataset)\n",
    "\n",
    "    return  datasets\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_train(dataset, len_new_data):\n",
    "    '''\n",
    "    Reduces an input set to a target size\n",
    "\n",
    "    Args:\n",
    "        dataset: original dataset\n",
    "        len_new_data: target length of new dataset\n",
    "    Returns:\n",
    "        sample of input dataset of target length\n",
    "    '''\n",
    "    len_kept_data = len(dataset) - len_new_data\n",
    "    new_train_set, _ = random_split(dataset, [len_kept_data, len(dataset)-len_kept_data])\n",
    "    return new_train_set\n",
    "\n",
    "def prepare_trainsets(snli_train_dataset, mnli_train_dataset, anli_training_datasets):\n",
    "    '''\n",
    "    Allocate data to each round of training.\n",
    "\n",
    "    Args:\n",
    "        snli, mnli trainsets\n",
    "        dicts of per round anli trainsets\n",
    "    Returns:\n",
    "        round1: sample of snli + mnli equal to sum(len(anli1), len(anli2), len(anli3))\n",
    "        subsequent rounds adds anli sequentially and subtracts the added anli round's \n",
    "        length from the snli+mnli portion of the previous round, so:\n",
    "        round2: reduced smnli sample + anli1\n",
    "        round3: further reduced smnli sample + anli2\n",
    "        round4: further reduced smnli sample + anli3\n",
    "    '''\n",
    "\n",
    "    # This function depends on the trainsets, so it is the only part of the notebook that is not exemplified at all by a toy run\n",
    "    \n",
    "    smnli = ConcatDataset([snli_train_dataset, mnli_train_dataset])\n",
    "\n",
    "    len_anli = 0\n",
    "    for round, dataset in anli_training_datasets.items():\n",
    "    \n",
    "        len_anli+=len(dataset)\n",
    "   \n",
    "    reduced_smnli1, _ = random_split(smnli, [len_anli, len(smnli)-len_anli])\n",
    "\n",
    "    round1 = reduced_smnli1\n",
    "\n",
    "    reduced_smnli2 = reduce_train(reduced_smnli1, len(anli_training_datasets['R1']))\n",
    "    round2 = ConcatDataset([reduced_smnli2, anli_training_datasets['R1']])\n",
    "\n",
    "    reduced_smnli3 = reduce_train(reduced_smnli2, len(anli_training_datasets['R2']))\n",
    "    round3 = ConcatDataset([reduced_smnli3, anli_training_datasets['R1'], anli_training_datasets['R2']])\n",
    "    \n",
    "\n",
    "    round4 = ConcatDataset([anli_training_datasets['R1'], anli_training_datasets['R2'], anli_training_datasets['R3'] ])\n",
    "\n",
    "    print(len(round1), len(round2), len(round3), len(round4)) # same length\n",
    "\n",
    "    return round1, round2, round3, round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_pred, y_test, classes, logfile = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Prints accuracy and macro f-score based on inputs.\n",
    "    Writes to file instead if logfile == truthy\n",
    "    Args:\n",
    "        y_pred: list of class predictions made by model\n",
    "        y_test: list of gold labels of evaluation set where y_test[i] corresponds to y_pred[i]\n",
    "        logfile: path to log. Must be in an existing directory\n",
    "    \"\"\"\n",
    "    # logfile is vestigial in final version. Nothing to change at the config-level to turn this on.\n",
    "    # function is not really meant to store final data, just to track mid-training whether the model is getting better\n",
    "\n",
    "    if logfile:\n",
    "        print(logfile)\n",
    "        print('Logging results to file instead of printing: see model directory')\n",
    "        resultspath = os.path.join(os.getcwd(), logfile) \n",
    "        print('Logging to',resultspath)\n",
    "        log = open(resultspath, 'a')\n",
    "        sys.stdout = log\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #p_scores = precision_score(y_test, y_pred, average = None, zero_division=0.0)\n",
    "    #r_scores = recall_score(y_test, y_pred, average = None, zero_division=0.0)\n",
    "    f_scores = f1_score(y_test, y_pred, average = None, zero_division=0.0)\n",
    "    #macro_p = precision_score(y_test, y_pred, average = \"macro\", zero_division=0.0)\n",
    "    #macro_r = recall_score(y_test, y_pred, average = \"macro\", zero_division=0.0)\n",
    "    macro_f = f1_score(y_test, y_pred, average = \"macro\", zero_division=0.0)\n",
    "\n",
    "    print()\n",
    "    print(\"accuracy is\", accuracy)\n",
    "    print()\n",
    "\n",
    "    for label, f_score in zip(classes, f_scores):\n",
    "        print(\"f-score for label '{}' is {}\".format(label, f_score))\n",
    "    print(\"macro f-score is\", macro_f)\n",
    "    print('------------------------------------------------')\n",
    "    if logfile:\n",
    "        log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_specifier, train_ds, dirpath, hyperparams, midtrain_eval=False):\n",
    "    '''\n",
    "    Trains model and saves a checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model_specifier: dict of huggingface classes for loading, see globals() -> model_selection\n",
    "        train_ds: Dataset of class NLIData\n",
    "        dirpath: path to where model will be saved\n",
    "        midtrain_eval: if truthy, should be NLIData dataset used for midtrain evaluation. Evaluation skipped if falsy\n",
    "    '''\n",
    "\n",
    "    model = model_specifier['sequence_classification'].from_pretrained(model_specifier['model_name'], num_labels=3).to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparams['lr'])\n",
    "    for epoch in range(hyperparams['epochs']):\n",
    "        print('Epoch', str(epoch+1)+':')\n",
    "        print()  \n",
    "        loader = DataLoader(dataset=train_ds,\n",
    "                        batch_size=hyperparams['batch_size'],\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate_fn)\n",
    "        loss_metrics = {}\n",
    "        total_loss = 0\n",
    "        for batch_id, batch in enumerate(tqdm.tqdm(loader, desc=\"Batches\")):\n",
    "            model.train()\n",
    "            outputs = model(batch['input_ids'].to(device),\n",
    "                            attention_mask=batch['attention_masks'].to(device),\n",
    "                            token_type_ids=batch['token_type_ids'].to(device),\n",
    "                            labels=batch['label_tensors'].to(device))\n",
    "            \n",
    "            loss, logits = outputs[:2] #pretrained bert includes the loss if labels are provided to the model,\n",
    "                                        # so I don't need to do that separately\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "  \n",
    "            total_loss += loss.item()\n",
    "            model.zero_grad()\n",
    "            del outputs, loss, logits\n",
    "\n",
    "        average_loss = total_loss/len(loader)\n",
    "        loss_metrics['total_loss'] = total_loss\n",
    "        loss_metrics['average_loss'] = average_loss           \n",
    "        print('Average loss for epoch:', average_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "        checkpoint_path = os.path.join(dirpath, 'e'+str(epoch+1))\n",
    "        model.save_pretrained(checkpoint_path)\n",
    "        tokenizer.save_pretrained(checkpoint_path)\n",
    "        loss_path = os.path.join(checkpoint_path, 'losses.json')\n",
    "        with open(loss_path, 'w') as f:\n",
    "            json.dump(loss_metrics, f)\n",
    "        models['e'+str(epoch+1)] = model\n",
    "        \n",
    "        if midtrain_eval:\n",
    "            print('Evaluating epoch {}.'.format(epoch+1))\n",
    "            # just printing some results to see if the model seems to be improving\n",
    "            test(eval_dataset=midtrain_eval, modelin=model, print_results=True)\n",
    "    return models\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_testing(dataset, modelpath):\n",
    "    # just some work I did for myself to ensure I got the entropy right, since different functions\n",
    "    # work somewhat differently and accept different input forms.\n",
    "    # end up using scipy entropy, base2\n",
    "\n",
    "    loader = DataLoader(dataset, collate_fn=collate_fn, batch_size=1)\n",
    "    model = model_specifier['sequence_classification'].from_pretrained(modelpath).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in loader:\n",
    "        outputs = model(batch['input_ids'].to(device),\n",
    "                        attention_mask=batch['attention_masks'].to(device),\n",
    "                        token_type_ids=batch['token_type_ids'].to(device),\n",
    "                    )\n",
    "        \n",
    "        # cnli includes entropy with base2 logarithm:\n",
    "        print()\n",
    "        print(type(batch))\n",
    "        cnli_ent = batch['unique_data'][0]['entropy']\n",
    "        print('cnli entropy, base2:',cnli_ent)\n",
    "        print()\n",
    "\n",
    "        # label count of the same example:\n",
    "        lc = batch['unique_data'][0]['label_count']\n",
    "\n",
    "        #from count to label % distribution:\n",
    "        dist = [count/100 for count in lc]\n",
    "\n",
    "        # DIY numpy entropy from dist\n",
    "        p = np.array(dist)\n",
    "        logp = np.log2(p)\n",
    "        numpy_ent1 = np.sum(-p*logp)\n",
    "\n",
    "        # numpy entropy from lc\n",
    "        p = np.array(lc)\n",
    "        logp = np.log2(p)\n",
    "        numpy_ent2 = np.sum(-p*logp)\n",
    "\n",
    "        print('numpy:')\n",
    "        print('np log2 entropy from dist:', numpy_ent1)\n",
    "        print('np log2 entropy from count:', numpy_ent2)\n",
    "        print()\n",
    "\n",
    "        p_tensor_dist = torch.Tensor(dist)\n",
    "        tensor_lc = torch.Tensor(lc)\n",
    "\n",
    "        pt_ent1 = Categorical(probs = p_tensor_dist).entropy()\n",
    "        pt_ent2 = Categorical(probs = tensor_lc).entropy()\n",
    "\n",
    "        print('pt, default=natlog:')\n",
    "        print('pt ent from dist:', pt_ent1)\n",
    "        print('pt ent from count:', pt_ent2)\n",
    "        print()\n",
    "\n",
    "        sp_ent_dist = entropy(dist)\n",
    "        sp_ent_lc = entropy(lc)\n",
    "        sp_ent_dist_b2 = entropy(dist, base=2)\n",
    "\n",
    "        print('scipy, default=natlog')\n",
    "        print('scipy entropy from dist:', sp_ent_dist)\n",
    "        print('scipy entropy from count:', sp_ent_lc)\n",
    "        print('scipy entropy from dist base2:', sp_ent_dist_b2)\n",
    "        print() \n",
    "\n",
    "        logits = outputs[0][0]\n",
    "        probs = torch.softmax(logits, -1).squeeze()\n",
    "\n",
    "        print('logits',logits)\n",
    "        print('probs',probs)\n",
    "        pt_ent_from_logits = Categorical(logits = p_tensor_dist).entropy()\n",
    "        pt_ent_from_softmax = Categorical(probs = p_tensor_dist).entropy()\n",
    "        np_probs = probs.cpu().detach().numpy()\n",
    "        print('np_probs',np_probs)\n",
    "        scipy_ent_from_logits = entropy(logits.cpu().detach().numpy())\n",
    "        scipy_ent_from_softmax = entropy(np_probs)\n",
    "        scipy_ent_from_logits_b2 = entropy(logits.cpu().detach().numpy(), base=2)\n",
    "        scipy_ent_from_softmax_b2 = entropy(np_probs, base=2)\n",
    "\n",
    "        p = np.array(np_probs)\n",
    "        log2p = np.log2(p)\n",
    "        logp = np.log(p) #nat log\n",
    "        model_numpy_entb2 = np.sum(-p*log2p)\n",
    "        model_numpy_natent = np.sum(-p*logp)\n",
    "\n",
    "        print('pt entropy from logits:', pt_ent_from_logits)\n",
    "        print('pt entropy from softmax:', pt_ent_from_softmax)\n",
    "        print('scipy entropy from logits:', scipy_ent_from_logits)\n",
    "        print('scipy entropy from softmax:', scipy_ent_from_softmax)\n",
    "        print('scipy entropy from logits, b2:', scipy_ent_from_logits_b2)\n",
    "        print('scipy entropy from softmax, b2:', scipy_ent_from_softmax_b2)\n",
    "        print()\n",
    "        print('model numpy entropy, natlog', model_numpy_natent)\n",
    "        print('model numpy entropy, b2', model_numpy_entb2)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(unique_data, logits):\n",
    "    '''\n",
    "    Computes model entropy and human entropy.\n",
    "\n",
    "    Arg:\n",
    "        unique_data: data from which to extract label distributions, unique structure per original set\n",
    "        logits: model output logits\n",
    "    '''\n",
    "    if 'label_count' in unique_data:\n",
    "        hum_dist = [c/100 for c in unique_data['label_count']]\n",
    "    else:\n",
    "        #print(unique_data)\n",
    "        hum_dist = [0, 0, 0]\n",
    "        lb_scales = unique_data['labels']\n",
    "\n",
    "        # Discretize the labels to make them interface in the model, by the same thresholds as\n",
    "        # in the original paper (Pavlick, Kwiatkowski, 2019).\n",
    "        # This means that while I don't make use of the grading scale, I still utilise the annotator variation.\n",
    "        # Also makes the NLI_val results more comparable to those from CNLI.\n",
    "        for l in lb_scales:\n",
    "            if l > 16.7:\n",
    "                hum_dist[0] = hum_dist[0]+1\n",
    "            elif l < -16.7:\n",
    "                hum_dist[2] = hum_dist[2]+1\n",
    "            else:\n",
    "                hum_dist[1] = hum_dist[1]+1\n",
    "        hum_dist = [c/sum(hum_dist) for c in hum_dist]\n",
    "    \n",
    "    hum_ent = entropy(hum_dist, base=2)\n",
    "\n",
    "    model_dist = torch.softmax(logits, -1).squeeze().cpu().detach().numpy()\n",
    "    model_ent = entropy(model_dist, base=2)  \n",
    "    return hum_ent, model_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(eval_dataset, modelpath=False, modelin=False, print_results=False):\n",
    "    '''\n",
    "    Evaluates model on one eval set.\n",
    "\n",
    "    Args:\n",
    "        eval_dataset: NLIData type dataset\n",
    "        modelpath: path to model, must be supplied if modelin is not. Will be used if both are provided\n",
    "        modelin: model object, must be supplied if modelpath is not. Will not be used if both are provided\n",
    "        print_results: prints accuracy and macro f-score if truthy\n",
    "    Returns:\n",
    "        lists of y_pred, y_test, human_entropies, model_entropies\n",
    "        element i of each list corresponds to the same example, except if eval set is NLI_val,\n",
    "        in which case y_test and y_pred are empty        \n",
    "    '''\n",
    "    # print_results is mostly for a mid-training overview. Full results come later\n",
    "\n",
    "    if not modelpath and not modelin:\n",
    "        print(\"Suppy either 'modelpath' or 'modelin' argument\")\n",
    "        return None    \n",
    "\n",
    "    loader = DataLoader(dataset=eval_dataset, collate_fn=collate_fn, batch_size=1)\n",
    "    if modelpath:\n",
    "        model = model_specifier['sequence_classification'].from_pretrained(modelpath).to(device)\n",
    "    else:\n",
    "        model = modelin\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    human_entropies = []\n",
    "    model_entropies = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if toy_run:\n",
    "            print('Toy run: stopping early')\n",
    "        for batch_id, batch in enumerate(tqdm.tqdm(loader)):\n",
    "            if toy_run:\n",
    "                if len(human_entropies) > 10:\n",
    "                    break\n",
    "            if 'label_tensors' in batch:\n",
    "                label_input = batch['label_tensors'].to(device)\n",
    "            else:\n",
    "                label_input = None # no gold labels in NLI_var\n",
    "\n",
    "            outputs = model(batch['input_ids'].to(device),\n",
    "                                attention_mask=batch['attention_masks'].to(device),\n",
    "                                token_type_ids=batch['token_type_ids'].to(device),\n",
    "                                labels=label_input)\n",
    "            \n",
    "            \n",
    "            if 'label_tensors' in batch:\n",
    "                loss, logits = outputs[:2]\n",
    "                pred = torch.argmax(logits)\n",
    "                gold = batch['label_tensors'][0][0]\n",
    "                y_test.append(gold.cpu())\n",
    "                y_pred.append(pred.cpu())\n",
    "            else:\n",
    "                logits = outputs[0]\n",
    "\n",
    "\n",
    "            hum_ent, model_ent = get_entropy(batch['unique_data'][0], logits)\n",
    "            human_entropies.append(hum_ent)\n",
    "            model_entropies.append(model_ent)\n",
    "\n",
    "    if print_results:\n",
    "        print_metrics(y_pred, y_test, classes)\n",
    "    \n",
    "    return y_pred, y_test, human_entropies, model_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical(data_dict):\n",
    "    '''\n",
    "    Plots a line graph based on input dict\n",
    "    '''\n",
    "    #if 'linestyle' in data_dict:\n",
    "    #    linestyle = data_dict['linestyle']\n",
    "    #else:\n",
    "    #    linestyle='-'\n",
    "    x = data_dict['x']\n",
    "    for y, label in zip(data_dict['y'], data_dict['legend_lables']):\n",
    "        # y is list of numerical data\n",
    "        # data_dict['y'] is a list of y, with each element = a graph line\n",
    "        # x is list of epochs (xticks)\n",
    "        # label is legend name\n",
    "        plt.plot(x, y, label=label, marker = 'o', ) #linestyle=linestyle\n",
    "    if 'ylim' in data_dict:\n",
    "        plt.ylim(data_dict['ylim'])\n",
    "    plt.legend()\n",
    "    plt.title(data_dict['title'], fontsize=16)\n",
    "    plt.savefig(data_dict['filepath'], bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganise_for_crossround(round_set_epoch):\n",
    "    '''\n",
    "    Changes input nested list shape from [rounds[sets[epochs]]] to [epochs[sets[rounds]]]\n",
    "    '''\n",
    "    # The loop unfurling order in full_eval is based on the needs of the intra-round plots (epochs=x, sets=legend labels, rounds=new figure).\n",
    "    # This causes a nesting issue for inter-round plots (epochs=new plot, sets=legend labels, rounds=x)\n",
    "    # Hence, this:\n",
    "\n",
    "    epoch_set_round = []\n",
    "\n",
    "    for e in range(len(round_set_epoch[0][0])):\n",
    "        set_data = []\n",
    "        for s in range(len(round_set_epoch[0])):\n",
    "            rounddata = [round_set_epoch[r][s][e] for r in range(len(round_set_epoch))]\n",
    "            set_data.append(rounddata)\n",
    "        epoch_set_round.append(set_data)\n",
    "        \n",
    "    return epoch_set_round\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_eval(model_specifier, modeldir, eval_sets):\n",
    "    '''\n",
    "    Evaluates each checkpoint of a model on all evaluation sets for accuracy and macro f1 (if available),\n",
    "    and entropy correlation with human distributions. Plots scatterplots per epoch per set for entropy correlation,\n",
    "    and lineplots for development across epochs within-round and across rounds.\n",
    "\n",
    "    Args:\n",
    "        model_specifier: id for huggingface model, see globals()\n",
    "        modeldir: path to model directory at the level of children to ./models\n",
    "        eval_sets: dict of evaluation sets\n",
    "        '''\n",
    "    \n",
    "    plot_data_dicts = []\n",
    "    overall_resultspath = os.path.join(modeldir, 'results')\n",
    "    if not os.path.exists(overall_resultspath):\n",
    "        os.mkdir(overall_resultspath)\n",
    "    cross_round_accuracies = []\n",
    "    cross_round_mfscores = []\n",
    "    cross_round_ent_corrs = []\n",
    "    rounds = []\n",
    "    epoch_names = []\n",
    "    print('Running evaluation for every epoch-checkpoint of every round on each set.')\n",
    "    for roundname in os.listdir(modeldir):\n",
    "        if 'json' in roundname or 'results' in roundname:\n",
    "            continue\n",
    "        rounds.append(roundname)\n",
    "        rounddir = os.path.join(modeldir, roundname)\n",
    "        round_resultspath = os.path.join(rounddir, 'results')\n",
    "        if not os.path.exists(round_resultspath):\n",
    "            os.mkdir(round_resultspath)\n",
    "        epochs = []\n",
    "        per_set_accs = []\n",
    "        per_set_mfs = []\n",
    "        per_set_entcorrs = []\n",
    "        for eval_set_name, eval_set in list(eval_sets.items()):        \n",
    "            per_epoch_accs =  []\n",
    "            per_epoch_mfs = []\n",
    "            per_epoch_entcorrs = []\n",
    "       \n",
    "            for epoch in os.listdir(rounddir):\n",
    "                epochdir = os.path.join(rounddir, epoch)\n",
    "                if 'results' in epoch: # checks if the current iteration is over a results directory at the round level (i.e sibling of 'e1' etc.)\n",
    "                                        # not to be mistaken with the next few lines of code,\n",
    "                                        # which creates a results directory for epoch-level results.\n",
    "                    continue\n",
    "                epoch_resultspath = os.path.join(epochdir, 'results')\n",
    "                if not os.path.exists(epoch_resultspath):\n",
    "                    os.mkdir(epoch_resultspath)\n",
    "\n",
    "                model =  model_specifier['sequence_classification'].from_pretrained(epochdir).to(device)\n",
    "                if len(epochs) < len(os.listdir(rounddir))-1: # Number of checkpoints per round\n",
    "                                                # could have just hardcoded 6, since that's the number of epochs I train with,\n",
    "                                                # but this is technically more flexible  \n",
    "                    epochs.append(epoch)\n",
    "\n",
    "                print('Getting scores for {} {} on {}.'.format(roundname, epoch, eval_set_name))\n",
    "                y_pred, y_test, human_entropies, model_entropies = test(modelin=model, eval_dataset=eval_set)\n",
    "                human_entropies = np.array(human_entropies)\n",
    "                model_entropies = np.array(model_entropies)\n",
    "                pearson_c = np.corrcoef(model_entropies, human_entropies)[0][1]\n",
    "                #r, p = pearsonr(model_entropies, human_entropies) # changed to np as it behaved somewhat better on toy runs\n",
    "                                                                    # still cannot rescue the obviously terrible toy models\n",
    "                            \n",
    "                per_epoch_entcorrs.append(pearson_c)\n",
    "                if not eval_set_name == 'NLIVariation':\n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    macro_f = f1_score(y_test, y_pred, average = \"macro\", zero_division=0.0)\n",
    "                    per_epoch_accs.append(accuracy)\n",
    "                    per_epoch_mfs.append(macro_f)\n",
    "\n",
    "                plt.scatter(human_entropies, model_entropies)\n",
    "        \n",
    "                # from https://pythonguides.com/matplotlib-best-fit-line/ and https://www.statology.org/line-of-best-fit-python/\n",
    "\n",
    "                a, b = np.polyfit(human_entropies, model_entropies, 1)\n",
    "                plt.plot(human_entropies, a*human_entropies+b, color='orange') \n",
    "                plt.title(\" \".join((eval_set_name, roundname, epoch, 'entropy with line of best fit')))\n",
    "                plt.xlabel('Human entropy')\n",
    "                plt.ylabel('Model entropy')\n",
    "                plt.annotate('r = {:.2f}'.format(pearson_c), xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "                plt.savefig(os.path.join(epoch_resultspath, eval_set_name+'-entropies'), bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "                \n",
    "            if not eval_set_name == 'NLIVariation':               \n",
    "                per_set_accs.append(per_epoch_accs)\n",
    "                per_set_mfs.append(per_epoch_mfs)\n",
    "            per_set_entcorrs.append(per_epoch_entcorrs)\n",
    "\n",
    "        # per round results dicts to be plugged into plot_categorical:    \n",
    "        plot_input_accuracy = {'y': per_set_accs, 'x': epochs, 'legend_lables': list(eval_sets.keys())[:2],\n",
    "                        'title': roundname+' Accuracy', 'filepath': os.path.join(round_resultspath, 'accuracy'), 'ylim': (0,1)}\n",
    "        plot_input_mf = {'y': per_set_mfs, 'x': epochs, 'legend_lables': list(eval_sets.keys())[:2],\n",
    "                        'title': roundname+' Macro F1', 'filepath': os.path.join(round_resultspath, 'macrof1'), 'ylim': (0,1)}\n",
    "        plot_input_ent = {'y': per_set_entcorrs, 'x': epochs, 'legend_lables': list(eval_sets.keys()),\n",
    "                        'title': roundname+' Model/human entropy correlation', 'filepath': os.path.join(round_resultspath, 'entropy_correlation'), 'ylim': (-1,1)}\n",
    "\n",
    "        plot_data_dicts.extend([plot_input_accuracy, plot_input_mf, plot_input_ent])\n",
    "\n",
    "        if not epoch_names: # just to have the number of epochs at the cross-round level I do this once:\n",
    "            epoch_names.extend(epochs)\n",
    "\n",
    "        cross_round_accuracies.append(per_set_accs)\n",
    "        cross_round_mfscores.append(per_set_mfs)\n",
    "        cross_round_ent_corrs.append(per_set_entcorrs)\n",
    "        \n",
    "        # Get metrics from last epoch for each set:\n",
    "        # I don't do this here, but this could instead fetch the result of a specified epoch,\n",
    "        # e.g., if I wanted to get specifically the best epoch\n",
    "\n",
    "        # for each metric:\n",
    "        # from structure [set1[e1,e2,e3,e4,e5,e6], set2[...]...] to [e1[set1, set2...], e2[...]...]\n",
    "\n",
    "\n",
    "    # final_epoch_acc/mf/ents are lists of lists whose outer elements are per round data and subelements of those elements are per test set\n",
    "    # data for the respective round. I want to plot metrics across rounds (so round number on X), and plot_categorical expects y to be\n",
    "    # list of lists where the INNER list corresponds to the x-ticks. So I invert:\n",
    "    # (could have avoided this by iterating over sets before over rounds, but my per-round plotting above has a different preference)\n",
    "\n",
    "    \n",
    "\n",
    "    #reorganised_acc = reorder_sublists_by_idx(cross_round_accuracies)\n",
    "    #reorganised_mf = reorder_sublists_by_idx(cross_round_mfscores)\n",
    "    #reorganised_ent = reorder_sublists_by_idx(cross_round_ent_corrs)\n",
    "\n",
    "    reorganised_acc = reorganise_for_crossround(cross_round_accuracies)\n",
    "    reorganised_mf = reorganise_for_crossround(cross_round_mfscores)\n",
    "    reorganised_ent = reorganise_for_crossround(cross_round_ent_corrs)\n",
    "\n",
    "\n",
    "    accpath = os.path.join(overall_resultspath, 'accuracy')\n",
    "    mfpath = os.path.join(overall_resultspath, 'macrof1')\n",
    "    entpath = os.path.join(overall_resultspath, 'entropy_correlation')\n",
    "\n",
    "    for p in (accpath, mfpath, entpath):\n",
    "        if not os.path.exists(p):\n",
    "            os.mkdir(p)\n",
    "\n",
    "\n",
    "    for i in range(len(epoch_names)):         \n",
    "        # plotdicts comparing epoch n across rounds\n",
    "        plot_input_accuracy = {'y': reorganised_acc[i], 'x': rounds, 'legend_lables': list(eval_sets.keys())[:2],\n",
    "                            'title': 'Epoch {} accuracy across rounds'.format(i+1),\n",
    "                            'filepath': os.path.join(accpath, epoch_names[i]), 'ylim': (0,1)}\n",
    "        plot_input_mf = {'y': reorganised_mf[i], 'x': rounds, 'legend_lables': list(eval_sets.keys())[:2],\n",
    "                            'title': 'Epoch {} macro F1 across rounds'.format(i+1),\n",
    "                            'filepath': os.path.join(mfpath, epoch_names[i]), 'ylim': (0,1)}\n",
    "        plot_input_ent = {'y': reorganised_ent[i], 'x': rounds, 'legend_lables': list(eval_sets.keys()), \n",
    "                          'title': 'Epoch {} model/human entropy correlation across rounds'.format(i+1),\n",
    "                          'filepath': os.path.join(entpath, epoch_names[i]), 'ylim': (-1,1)} #'linestyle': 'None',\n",
    "\n",
    "        plot_data_dicts.extend([plot_input_accuracy, plot_input_mf, plot_input_ent])\n",
    "    \n",
    "    for plot_data in plot_data_dicts:\n",
    "        plot_categorical(plot_data)                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    if (train_eval_mode == 'eval') or toy_run:\n",
    "        cnli_eval_datasets, NLI_var_eval_dataset = get_datasets()\n",
    "    else:\n",
    "        snli_train_dataset, mnli_train_dataset, anli_training_datasets, cnli_eval_datasets, NLI_var_eval_dataset = get_datasets()\n",
    "        \n",
    "    if not train_eval_mode == 'eval':\n",
    "        if toy_run:\n",
    "            training_rounds = list(cnli_eval_datasets.values())\n",
    "            print('Toy run:')\n",
    "            print('Training on toy parameters')\n",
    "            print(hp)\n",
    "            print('Training on', list(cnli_eval_datasets.keys()))\n",
    "        else:\n",
    "            r1train, r2train, r3train, r4train = prepare_trainsets(snli_train_dataset, mnli_train_dataset, anli_training_datasets)\n",
    "            training_rounds = (r1train, r2train, r3train, r4train)\n",
    "            print('Running full training on all four rounds. Hyperparameters:')\n",
    "            print(hp)\n",
    "\n",
    "        \n",
    "        newpath = os.path.join(os.getcwd(), 'models')\n",
    "        if not os.path.exists(newpath):\n",
    "            os.mkdir(newpath)\n",
    "        \n",
    "        # the modelname is the name of a subdirectory in ./models, which will have its own filesystem:\n",
    "            # a directory 'r'+n per round and a sibling 'results' directory for cross-round results\n",
    "            # each 'r' dir has subdirs 'e'+n per epoch with a sibling 'results' directory for in-round results\n",
    "            # each 'e' dir has various files, mostly huggingface-style model files (safetensors removed for git submission due to size),\n",
    "                # and a 'results' directory for per-epoch results\n",
    "        \n",
    "        if manual_modelname:\n",
    "            dirname = manual_modelname\n",
    "        elif toy_run:\n",
    "            dirname = 'toy-models'\n",
    "        else:\n",
    "            modelname = model_specifier['model_name']\n",
    "            batch_size_str = 'bs'+str(hp['batch_size'])\n",
    "            epochs_str = 'eps'+str(hp['epochs'])\n",
    "            lr_str = 'lr'+str(hp['lr'])\n",
    "            dirname = '-'.join((modelname, batch_size_str, epochs_str, lr_str))\n",
    "        \n",
    "        dirpath = os.path.join(newpath, dirname)\n",
    "\n",
    "        if overwrite:\n",
    "            if os.path.exists(dirpath):\n",
    "                shutil.rmtree(dirpath)\n",
    "        else:\n",
    "            num = 2\n",
    "            dirpath_base = dirpath\n",
    "            while os.path.exists(dirpath):\n",
    "                dirpath = dirpath_base+'_'+str(num)\n",
    "                num+=1\n",
    "\n",
    "        os.mkdir(dirpath)\n",
    "\n",
    "        hppath = os.path.join(dirpath, 'hyperparameters.json')\n",
    "        \n",
    "        with open(hppath, 'w') as f:\n",
    "                json.dump(hp, f)\n",
    "        \n",
    "        if torch.cuda.is_available() or toy_run:          \n",
    "            for round_num, round_data in enumerate(training_rounds):\n",
    "                roundnum_str = 'r'+str(round_num+1)\n",
    "                roundpath = os.path.join(dirpath, roundnum_str)\n",
    "                os.mkdir(roundpath)\n",
    "                resultspath = os.path.join(roundpath, 'results')\n",
    "                os.mkdir(resultspath)\n",
    "                    #dirname = '-'.join((roundnum_str, modelname, batch_size_str, epochs_str, lr_str))\n",
    "\n",
    "                #dirpath = os.path.join(newpath, dirname)\n",
    "                #os.mkdir(dirpath)\n",
    "                print()\n",
    "                print('-------------------------------')\n",
    "                print('Training', roundnum_str)\n",
    "                round_models = train(model_specifier, round_data, roundpath, hp, midtrain_eval=cnli_eval_datasets['chaosNLI_snli'])\n",
    "                print()\n",
    "                #models[roundnum_str] = round_models\n",
    "        else:\n",
    "            print('Cuda unavailable. Set toy_run to true or try again with cuda available.')\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        \n",
    "    if not train_eval_mode == 'train':\n",
    "        eval_sets = {'ChaosNLI-MNLI': cnli_eval_datasets['chaosNLI_mnli_m'], 'ChaosNLI-SNLI': cnli_eval_datasets['chaosNLI_snli'], 'NLIVariation': NLI_var_eval_dataset}\n",
    "\n",
    "        try:\n",
    "            modeldir = dirpath # works if model has been trained in the same session, i.e., train_eval_mode is not 'eval'\n",
    "            print('Evaluating just-trained model, saving results to {} and its subdirectories'.format(modeldir))\n",
    "        except:\n",
    "            if manual_modelname:\n",
    "                dirpath = os.path.join('./models', manual_modelname)\n",
    "            else:\n",
    "                dirpath = os.path.join('./models', 'toy_models') # default: this will be created by one as-submitted training mode run with no changes to config/code\n",
    "\n",
    "            if os.path.exists(dirpath):\n",
    "                modeldir = dirpath\n",
    "                print('Evaluating {}\\nFind results in this directory and its subdirectories'.format(dirpath))\n",
    "                print()\n",
    "            else:\n",
    "                print('Model directory not found.')\n",
    "                print(\"Try training a new model or give the name of an existing model (see ./models/) to 'manual_modelname' in the config file\")\n",
    "                print('Note: the preexisting model directory does not have loadable models.')\n",
    "                return None\n",
    "        \n",
    "\n",
    "        full_eval(model_specifier, modeldir, eval_sets)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CNLI dataframes...\n",
      "Getting NLI variation dataframe...\n",
      "Transforming CNLI data...\n",
      "\t chaosNLI_mnli_m...\n",
      "\t chaosNLI_snli...\n",
      "Transforming NLI variation data...\n",
      "Toy run:\n",
      "Training on toy parameters\n",
      "{'batch_size': 4, 'epochs': 2, 'lr': 0.01}\n",
      "Training on ['chaosNLI_mnli_m', 'chaosNLI_snli']\n",
      "\n",
      "-------------------------------\n",
      "Training r1\n",
      "Epoch 1:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 400/400 [00:25<00:00, 15.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 2.355997349500831\n",
      "\n",
      "Evaluating epoch 1.\n",
      "Toy run: stopping early\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1514 [00:00<00:22, 68.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.36363636363636365\n",
      "\n",
      "f-score for label 'entailment' is 0.5333333333333333\n",
      "f-score for label 'neutral' is 0.0\n",
      "f-score for label 'contradiction' is 0.0\n",
      "macro f-score is 0.17777777777777778\n",
      "------------------------------------------------\n",
      "Epoch 2:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 400/400 [00:25<00:00, 15.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 2.177538239993155\n",
      "\n",
      "Evaluating epoch 2.\n",
      "Toy run: stopping early\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1514 [00:00<00:22, 68.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.09090909090909091\n",
      "\n",
      "f-score for label 'entailment' is 0.0\n",
      "f-score for label 'neutral' is 0.0\n",
      "f-score for label 'contradiction' is 0.16666666666666666\n",
      "macro f-score is 0.05555555555555555\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "Training r2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 379/379 [00:23<00:00, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 2.4032738186605767\n",
      "\n",
      "Evaluating epoch 1.\n",
      "Toy run: stopping early\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1514 [00:00<00:22, 66.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.09090909090909091\n",
      "\n",
      "f-score for label 'entailment' is 0.0\n",
      "f-score for label 'neutral' is 0.0\n",
      "f-score for label 'contradiction' is 0.16666666666666666\n",
      "macro f-score is 0.05555555555555555\n",
      "------------------------------------------------\n",
      "Epoch 2:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:  69%|██████▊   | 260/379 [00:15<00:07, 16.48it/s]"
     ]
    }
   ],
   "source": [
    "device, model_specifier, classes, alt_classnames, tokenizer, hp, toy_run, train_eval_mode, manual_modelname, overwrite = globals()\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
