{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, random_split\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions.continuous_bernoulli import ContinuousBernoulli\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import shutil\n",
    "#from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "#from torch.utils.data import Dataset#, Dataloader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification, AutoModel, logging\n",
    "logging.set_verbosity_error()\n",
    "import tqdm\n",
    "import sys\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import entropy, pearsonr\n",
    "from scipy.special import kl_div\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    '''\n",
    "    Sets seed for random, np, torch\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def globals():\n",
    "    '''\n",
    "    Returns the following parameters to be set globally:\n",
    "        device, model_specifier, classes, alt_classnames, tokenizer, hyperparams, toy_run\n",
    "    '''\n",
    "    # To avoid having to pass the same variables over and over, I set some global ones here. Keeps them in the same place\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    #args = parser.parse_args(args=[])\n",
    "    parser.add_argument(\"-c\", \"--config\", help=\"configuration file\", default=\"config.json\")\n",
    "    args = parser.parse_args(args=[])\n",
    "    config = json.load(open(args.config))\n",
    "\n",
    "    set_seed(config[\"seed\"])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(config[\"device\"])\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    model_selection = {\n",
    "    'bert_base': {\n",
    "        \"model_name\": \"bert-base-uncased\",\n",
    "        \"tokenizer\": BertTokenizer,\n",
    "        \"sequence_classification\": BertForSequenceClassification,\n",
    "    },\n",
    "    \n",
    "    'bert_large': {\n",
    "        \"model_name\": \"bert-large-uncased\",\n",
    "        \"tokenizer\": BertTokenizer,\n",
    "        \"sequence_classification\": BertForSequenceClassification,\n",
    "    },\n",
    "    'roberta_large': {\n",
    "        \"model_name\": \"roberta-large\",\n",
    "        \"tokenizer\": RobertaTokenizer,\n",
    "        \"sequence_classification\": RobertaForSequenceClassification,\n",
    "    }}\n",
    "\n",
    "    model_specifier = model_selection[config[\"model_id\"]]\n",
    "    classes = {'entailment': torch.as_tensor(0), 'neutral': torch.as_tensor(1), 'contradiction': torch.as_tensor(2)}\n",
    "    alt_classnames = {'e': 'entailment', 'n': 'neutral', 'c': 'contradiction' }\n",
    "    tokenizer = model_specifier['tokenizer'].from_pretrained(model_specifier['model_name'])\n",
    "    toy_run = config['toy_run']\n",
    "    if config['manual_hyperparams']:\n",
    "        hyperparams = config['manual_hyperparams']\n",
    "    elif toy_run:\n",
    "        hyperparams = {\n",
    "        'batch_size': 4,\n",
    "        'epochs': 2,\n",
    "        'lr' : 0.01\n",
    "        }\n",
    "    else:\n",
    "        hyperparams = {\n",
    "        'batch_size': 32,\n",
    "        'epochs': 6,\n",
    "        'lr' : 5e-5\n",
    "        }\n",
    "        \n",
    "\n",
    "    return device, model_specifier, classes, alt_classnames, tokenizer, hyperparams, toy_run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device, model_specifier, classes, alt_classnames, tokenizer, hyperparams, toy_run = globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get NLI variation:\n",
    "# evaluation will assume this exists \n",
    "\n",
    "nli_var_url = \"https://raw.githubusercontent.com/epavlick/NLI-variation-data/refs/heads/master/sentence-pair-analysis/preprocessed-data.jsonl\"\n",
    "nli_var_r = requests.get(nli_var_url)\n",
    "if not os.path.exists(\"./data/NLI_variation\"):\n",
    "        os.mkdir(\"./data/NLI_variation\")\n",
    "if not os.path.exists(\"./data/NLI_variation/NLI_variation_data.jsonl\"):\n",
    "    with open(\"./data/NLI_variation/NLI_variation_data.jsonl\", \"wb\") as f:\n",
    "        f.write(nli_var_r.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_headers(df):\n",
    "    '''\n",
    "    Normalises the column names that the different datasets come with.\n",
    "\n",
    "    Arg:\n",
    "        pandas dataframe based on any dataset used in this project\n",
    "    Returns:\n",
    "        Column-normalised dataframe\n",
    "    '''\n",
    "\n",
    "    #https://stackoverflow.com/questions/55715572/renaming-column-in-pandas-dataframe-if-condition-is-met, modified\n",
    "    \n",
    "    # there are potential data variations for which the following would be semantically incorrect, but it works for my data\n",
    "    df = df\n",
    "    alt_column_names = {'premise': ['context', 'sentence1'], 'hypothesis': ['statement', 'sentence2'], 'label': ['gold_label', 'labels', 'majority_label']}\n",
    "    df.columns = ['premise' if any(k == x for k in alt_column_names['premise']) else x for x in df]\n",
    "    df.columns = ['hypothesis' if any(k == x for k in alt_column_names['hypothesis']) else x for x in df]\n",
    "    df.columns = ['label' if any(k == x for k in alt_column_names['label']) else x for x in df]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_dict(setname):\n",
    "    '''\n",
    "    Converts raw dataset files into pandas dataframes\n",
    "    Arg: simple setname string, e.g., 'anli' for Adversarial NLI\n",
    "    Returns: dict of dataframes for the input dataset\n",
    "    '''\n",
    "    df_dict = {}\n",
    "    dirpath = './data/' + setname\n",
    "    walking = os.walk(dirpath)\n",
    "\n",
    "    for root, dirs, files in walking:\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            name, extension = os.path.splitext(full_path)\n",
    "            if extension == '.jsonl':\n",
    "                df = pd.read_json(path_or_buf=full_path, lines=True)\n",
    "                if setname == 'chaosNLI':\n",
    "                    cnli_examples = pd.json_normalize(df['example']) # p, h, label are in a single column. This makes a df with that column unpacked\n",
    "                    cnli_examples = cnli_examples.drop(['uid'], axis=1) # uid is in now redundantly in both the unpacked 'example' column and in the original df\n",
    "                    df = df.drop(['example'], axis=1)\n",
    "                    df = df.join(cnli_examples)\n",
    "            elif extension == '.csv':\n",
    "                df = pd.read_csv(filepath_or_buffer=full_path)\n",
    "            else:\n",
    "                continue # skips e.g. text files\n",
    "\n",
    "            df = normalise_headers(df)\n",
    "            \n",
    "            #anli has a one-deeper filesystem (per-round datasets), so the returned dict is also deeper\n",
    "            if setname == 'anli':\n",
    "                round, subset = name.split('/')[-2:]\n",
    "                if round not in df_dict:\n",
    "                    df_dict[round] = {}\n",
    "                if subset not in df_dict[round]:\n",
    "                    df_dict[round][subset] = df\n",
    "            else:\n",
    "                subset = name.split('/')[-1]\n",
    "                #print(subset)\n",
    "                df_dict[subset] = df\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def drop_unlabeled(df):\n",
    "    # snli has some unlabeled examples\n",
    "    df.drop(df.loc[df['label']=='-'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop_unlabeled(snli['snli_train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#s_id_list = list(snli['snli_train']['pairID'])\n",
    "#m_id_list = list(mnli['multinli_train']['pairID'])\n",
    "\n",
    "def get_NLIvar_duplicate_indices(trainset_str='snli', comparative_print=False):\n",
    "    # The NLI variation dataset partially uses inference pairs from SNLI and MNLI training data.\n",
    "    # I remove duplicates from the training sets since the effect on the size is negligible (~100 pairs\n",
    "    # per set, out of hundreds of thousands). Doing it the other way around (remove from NLIvar) would\n",
    "    # remove a notable chunk out of this eval set\n",
    "\n",
    "    # \n",
    "\n",
    "    # chaosNLI also draws from MNLI and SNLI, but from the dev sets, so there is no data contamination\n",
    "\n",
    "    # call with 'snli' (default) or 'mnli'\n",
    "\n",
    "    #\n",
    "    if trainset_str == 'snli':\n",
    "        trainset = snli['snli_train']\n",
    "        trainset_id_list = list(snli['snli_train']['pairID'])\n",
    "    elif trainset_str == 'mnli':\n",
    "        trainset = mnli['multinli_train']\n",
    "        trainset_id_list = list(mnli['multinli_train']['pairID'])\n",
    "    else:\n",
    "        return None\n",
    "    #s_id_list = list(snli['snli_train']['pairID'])\n",
    "    #m_id_list = list(mnli['multinli_train']['pairID'])\n",
    "\n",
    "    nli_var_filtered = nli_var.loc[nli_var['task'] == trainset_str]\n",
    "    nli_var_ids = list(nli_var_filtered['id'])\n",
    "    #nli_var_mnli = nli_var.loc[nli_var['task'] == 'mnli']\n",
    "    #nli_var_mnli_ids = list(nli_var_mnli['id'])\n",
    "    count = 0\n",
    "    duplicate_var_indices = []\n",
    "    duplicate_train_indices = []\n",
    "    for item in nli_var_ids:\n",
    "        if item in trainset_id_list:\n",
    "            count +=1\n",
    "            varidx = nli_var_filtered.loc[nli_var_filtered['id'] == item].index[0]\n",
    "            trainsetidx = trainset['pairID'].loc[trainset['pairID'] == item].index[0]\n",
    "            duplicate_var_indices.append(varidx)\n",
    "            duplicate_train_indices.append(trainsetidx)\n",
    "\n",
    "            if comparative_print:\n",
    "                # for checking that there are no discrepancies, e.g. that the given ids in NLI variation actually represents\n",
    "                # the pairIDs in snli and mnli       \n",
    "                print(nli_var_filtered.premise[varidx])\n",
    "                print(nli_var_filtered.hypothesis[varidx])\n",
    "                print(nli_var_filtered.label[varidx])\n",
    "                print(nli_var_filtered.id[varidx])\n",
    "                print()\n",
    "                print(trainset.premise[trainsetidx])\n",
    "                print(trainset.hypothesis[trainsetidx])\n",
    "                print(trainset.label[trainsetidx])\n",
    "                print(trainset.pairID[trainsetidx])\n",
    "                print('--------------------------')\n",
    "    return duplicate_train_indices\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duplicate_snli_indices = get_NLIvar_duplicate_indices('snli')\n",
    "duplicate_mnli_indices = get_NLIvar_duplicate_indices('mnli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels = []\n",
    "for dataset in sets:\n",
    "    for name, df in dataset.items():\n",
    "        labelset = set(df['label'])\n",
    "        labels.append((name, labelset))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#self.class_to_idx = {c: idx for idx, c in enumerate(self.classes)}\n",
    "\n",
    "classes = {'entailment': torch.as_tensor(0), 'neutral': torch.as_tensor(1), 'contradiction': torch.as_tensor(2)}\n",
    "alt_classnames = {'e': 'entailment', 'n': 'neutral', 'c': 'contradiction' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_selection = {'bert_base': {\n",
    "        \"model_name\": \"bert-base-uncased\",\n",
    "        \"tokenizer\": BertTokenizer,\n",
    "        \"sequence_classification\": BertForSequenceClassification,\n",
    "        \"padding_token_value\": 0,\n",
    "        \"padding_segement_value\": 0,\n",
    "        \"padding_att_value\": 0,\n",
    "        \"do_lower_case\": True,\n",
    "        \"internal_model_name\": \"bert\",\n",
    "        'insight_supported': True,\n",
    "    },\n",
    "    \n",
    "    'bert_large': {\n",
    "        \"model_name\": \"bert-large-uncased\",\n",
    "        \"tokenizer\": BertTokenizer,\n",
    "        \"sequence_classification\": BertForSequenceClassification,\n",
    "        \"padding_token_value\": 0,\n",
    "        \"padding_segement_value\": 0,\n",
    "        \"padding_att_value\": 0,\n",
    "        \"do_lower_case\": True,\n",
    "        \"internal_model_name\": \"bert\",\n",
    "        'insight_supported': True,\n",
    "    },\n",
    "'roberta_large': {\n",
    "        \"model_name\": \"roberta-large\",\n",
    "        \"tokenizer\": RobertaTokenizer,\n",
    "        \"sequence_classification\": RobertaForSequenceClassification,\n",
    "        \"padding_segement_value\": 0,\n",
    "        \"padding_att_value\": 0,\n",
    "        \"internal_model_name\": \"roberta\",\n",
    "        'insight_supported': True,\n",
    "    }}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#change model here\n",
    "model_specifier = model_selection['bert_base']\n",
    "tokenizer = model_specifier['tokenizer'].from_pretrained(model_specifier['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_encoded_input(tokenizer, df, max_length=None):\n",
    "    '''\n",
    "    Tokenizes premise and hypothesis (including special tokens).\n",
    "    Adds input ids, token type ids (token types mark belonging to premise or hypothesis), and attention mask to dataframe.\n",
    "    Args:\n",
    "        tokenizer\n",
    "        df to modify\n",
    "        max_length: max allowed sequence length\n",
    "    '''\n",
    "    # do not actually use max_length and there is no way to change this in the config, but it is something that could be wanted so I left it in\n",
    "\n",
    "    if 'input_encoding' in df.columns:\n",
    "        # not necessary if notebook is run like a script, but this safeguards against some notebook-type sequencing errors\n",
    "        # where an already encoding-modified df is input.\n",
    "        return df\n",
    "    \n",
    "    tokenizer = tokenizer\n",
    "    new_df = df\n",
    "    premises = list(df['premise'])\n",
    "    hypotheses = list(df['hypothesis'])\n",
    "    \n",
    "    encoded_pairs = [tokenizer.encode_plus(p, h, max_length=max_length, return_token_type_ids=True, truncation=True)\n",
    "                       for p, h in zip(premises, hypotheses)]\n",
    "    tensor_pairs = [{'input_ids': torch.as_tensor(encoded_pair['input_ids']),\n",
    "                      'token_type_ids': torch.as_tensor(encoded_pair['token_type_ids']),\n",
    "                      'attention_mask': torch.as_tensor(encoded_pair['attention_mask'])} \n",
    "                      for encoded_pair in encoded_pairs]\n",
    "    new_df.insert(0, 'input_encoding', tensor_pairs)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_encoded_labels(df):\n",
    "    '''\n",
    "    Adds tensor version of labels to dataframe\n",
    "    '''    \n",
    "    if 'label_tensor' in df.columns:\n",
    "        return df\n",
    "    new_df = df\n",
    "    encoded_labels = [classes[lb] if lb in classes else classes[alt_classnames[lb]] for lb in list(df.label)]\n",
    "    encoded_labels = torch.as_tensor(encoded_labels)\n",
    "    \n",
    "    new_df.insert(1, 'label_tensor', encoded_labels)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_encoded_df(tokenizer, df):\n",
    "    updated_df = df\n",
    "    updated_df = add_encoded_input(tokenizer, updated_df)\n",
    "    updated_df = add_encoded_labels(updated_df)\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = bert_large['tokenizer'].from_pretrained(bert_large['model_name'])\n",
    "                                                              #cache_dir=str(config.PRO_ROOT / \"trans_cache\"),\n",
    "                                                              #do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tokenizer = bert_large['tokenizer']\n",
    "for pr, hy in zip(p,h):\n",
    "    enc_p = tokenizer.encode_plus(pr, hy, max_length=None, return_token_type_ids=True, Truncation=True, return_tensors='pt') #, truncation=True\n",
    "    #encoding = tokenizer.encode_plus(pr, hy, truncation=True, return_token_type_ids=True, padding=True, return_tensors='pt')\n",
    "    print(pr)\n",
    "    print(hy)\n",
    "    print(enc_p)\n",
    "    print()\n",
    "    #print(encoding)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIData(Dataset):\n",
    "    '''\n",
    "    Subclass of pytorch Dataset\n",
    "    '''\n",
    "    def __init__(self, dataframe, require_label=True):\n",
    "\n",
    "        '''\n",
    "        args:\n",
    "            dataframe: pandas dataframe in style of get_df_dict output\n",
    "            require_label: bool indicating whether dataset has a discrete gold label\n",
    "\n",
    "        '''\n",
    "        # NLI variation does not have a discrete label, instead using a label distribution for each example\n",
    "\n",
    "        df = add_encoded_input(tokenizer, dataframe)\n",
    "        general_data = ['input_encoding', 'premise', 'hypothesis']\n",
    "        if require_label:\n",
    "            df = add_encoded_labels(df)\n",
    "            general_data = ['input_encoding', 'label_tensor', 'premise', 'hypothesis', 'label']\n",
    "        self.set_specific_data = df.drop(general_data, axis=1)\n",
    "        # set_specific_data is a bit of a garbage dump category that serves the purpose of making this class able to handle the diverse\n",
    "        # categories in the input sets\n",
    "\n",
    "        self.df = df\n",
    "        self.require_label = require_label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        out_dict = {'input_encoding': row['input_encoding'],\n",
    "                    'premise': row['premise'],\n",
    "                    'hypothesis': row['hypothesis'],\n",
    "                    'input_length': len(row['input_encoding']['input_ids']),\n",
    "                    'unique_data': self.set_specific_data.iloc[idx]\n",
    "                    }\n",
    "        if self.require_label:\n",
    "            out_dict['label'] = row['label']\n",
    "            out_dict['label_tensor'] = [row['label_tensor']]\n",
    "\n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_data):\n",
    "    \"\"\"\n",
    "    Collate function for pytorch dataloader\n",
    "    \"\"\"\n",
    "    # mainly used for padding\n",
    "\n",
    "    input_ids = [example['input_encoding']['input_ids'] for example in batch_data]\n",
    "    token_type_ids = [example['input_encoding']['token_type_ids'] for example in batch_data]\n",
    "    attention_masks = [example['input_encoding']['attention_mask'] for example in batch_data]\n",
    "\n",
    "    p_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    p_token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0)\n",
    "    p_attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    unique_data = [example['unique_data'] for example in batch_data]\n",
    "   \n",
    "    batch = {'input_ids': p_input_ids, 'token_type_ids': p_token_type_ids, 'attention_masks': p_attention_masks,\n",
    "             'unique_data': unique_data}\n",
    "    \n",
    "    if 'label_tensor' in batch_data[0]:\n",
    "        label_tensors = [example['label_tensor'] for example in batch_data]\n",
    "        label_tensors = torch.as_tensor(label_tensors)  #'label_tensors': label_tensors\n",
    "        batch['label_tensors'] = label_tensors\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_specifier, train_ds, dirpath, hyperparams, midtrain_eval=False):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "\n",
    "    model = model_specifier['sequence_classification'].from_pretrained(model_specifier['model_name'], num_labels=3).to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=hyperparams['lr'])\n",
    "    for epoch in range(hyperparams['epochs']):\n",
    "        print('Epoch', str(epoch+1)+':')\n",
    "        print()\n",
    "                \n",
    "        loader = DataLoader(dataset=train_ds,\n",
    "                        batch_size=hyperparams['batch_size'],\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate_fn)\n",
    "        loss_metrics = {}\n",
    "        total_loss = 0\n",
    "        for batch_id, batch in enumerate(tqdm.tqdm(loader, desc=\"Batches\")):\n",
    "            model.train()\n",
    "    \n",
    "\n",
    "\n",
    "            outputs = model(batch['input_ids'].to(device),\n",
    "                            attention_mask=batch['attention_masks'].to(device),\n",
    "                            token_type_ids=batch['token_type_ids'].to(device),\n",
    "                            labels=batch['label_tensors'].to(device))\n",
    "            \n",
    "            loss, logits = outputs[:2] #pretrained bert includes the loss if labels are provided to the model,\n",
    "                                        # so I don't need to do that separately\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "  \n",
    "            total_loss += loss.item()\n",
    "            model.zero_grad()\n",
    "            del outputs, loss, logits\n",
    "\n",
    "        average_loss = total_loss/len(loader)\n",
    "        loss_metrics['total_loss'] = total_loss\n",
    "        loss_metrics['average_loss'] = average_loss           \n",
    "        print('Average loss for epoch:', average_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "        checkpoint_path = os.path.join(dirpath, 'e'+str(epoch+1))\n",
    "        model.save_pretrained(checkpoint_path)\n",
    "        tokenizer.save_pretrained(checkpoint_path)\n",
    "        loss_path = os.path.join(checkpoint_path, 'losses.json')\n",
    "        with open(loss_path, 'w') as f:\n",
    "            json.dump(loss_metrics, f)\n",
    "        models['e'+str(epoch+1)] = model\n",
    "        \n",
    "        if midtrain_eval:\n",
    "            print('Evaluating epoch {}. See associated results '.format(epoch+1))\n",
    "            test(modelin=model, device=device, eval_dataset=cnli_eval_datasets['chaosNLI_mnli_m'])\n",
    "    #return model, ipi, li, am, lbs\n",
    "    return models\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model, ipi, li, am, lbs = train(model_specifier, cnli_eval_datasets['chaosNLI_mnli_m'], 'lalala', hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#model, ipi, li, am, lbs = train(model, cnli_eval_datasets['chaosNLI_mnli_m'], 'lalala', hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loader = DataLoader(dataset=cnli_eval_datasets['chaosNLI_mnli_m'],\n",
    "                        batch_size=1,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "outputs_list = []\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch in enumerate(tqdm.tqdm(loader, desc=\"Batches\")):   \n",
    "                outputs = model(batch['input_ids'],\n",
    "                                attention_mask=batch['attention_masks'],\n",
    "                                token_type_ids=batch['token_type_ids'],\n",
    "                                labels=batch['label_tensors'].to(device))\n",
    "                outputs_list.append(outputs)\n",
    "                #print(batch['premise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "cnt = {}\n",
    "for l in li:\n",
    "    if l == 'BATCHBREAK':\n",
    "        print('batchbreak')\n",
    "    else:\n",
    "        am = int(torch.argmax(l))\n",
    "        if am not in cnt:\n",
    "            cnt[am] = 0\n",
    "        cnt[am] += 1\n",
    "        print(torch.argmax(l))\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "\n",
    "    \n",
    "    def get_NLIvar_duplicate_indices(trainset_str='snli', comparative_print=False):\n",
    "        # The NLI variation dataset partially uses inference pairs from SNLI and MNLI training data.\n",
    "        # I remove duplicates from the training sets since the effect on the size is negligible (~100 pairs\n",
    "        # per set, out of hundreds of thousands). Doing it the other way around (remove from NLIvar) would\n",
    "        # remove a notable chunk out of this eval set\n",
    "\n",
    "        # chaosNLI also draws from MNLI and SNLI, but from the dev sets, so there is no data contamination\n",
    "\n",
    "        # call with 'snli' (default) or 'mnli'\n",
    "        if trainset_str == 'snli':\n",
    "            trainset = snli['snli_train']\n",
    "            trainset_id_list = list(snli['snli_train']['pairID'])\n",
    "        elif trainset_str == 'mnli':\n",
    "            trainset = mnli['multinli_train']\n",
    "            trainset_id_list = list(mnli['multinli_train']['pairID'])\n",
    "        else:\n",
    "            return None\n",
    "        #s_id_list = list(snli['snli_train']['pairID'])\n",
    "        #m_id_list = list(mnli['multinli_train']['pairID'])\n",
    "\n",
    "        nli_var_filtered = nli_var.loc[nli_var['task'] == trainset_str]\n",
    "        nli_var_ids = list(nli_var_filtered['id'])\n",
    "        #nli_var_mnli = nli_var.loc[nli_var['task'] == 'mnli']\n",
    "        #nli_var_mnli_ids = list(nli_var_mnli['id'])\n",
    "        count = 0\n",
    "        duplicate_var_indices = []\n",
    "        duplicate_train_indices = []\n",
    "        for item in nli_var_ids:\n",
    "            if item in trainset_id_list:\n",
    "                count +=1\n",
    "                varidx = nli_var_filtered.loc[nli_var_filtered['id'] == item].index[0]\n",
    "                trainsetidx = trainset['pairID'].loc[trainset['pairID'] == item].index[0]\n",
    "                duplicate_var_indices.append(varidx)\n",
    "                duplicate_train_indices.append(trainsetidx)\n",
    "\n",
    "                if comparative_print:\n",
    "                    # for checking that there are no discrepancies, e.g. that the given ids in NLI variation actually represents\n",
    "                    # the pairIDs in snli and mnli       \n",
    "                    print(nli_var_filtered.premise[varidx])\n",
    "                    print(nli_var_filtered.hypothesis[varidx])\n",
    "                    print(nli_var_filtered.label[varidx])\n",
    "                    print(nli_var_filtered.id[varidx])\n",
    "                    print()\n",
    "                    print(trainset.premise[trainsetidx])\n",
    "                    print(trainset.hypothesis[trainsetidx])\n",
    "                    print(trainset.label[trainsetidx])\n",
    "                    print(trainset.pairID[trainsetidx])\n",
    "                    print('--------------------------')\n",
    "        return duplicate_train_indices\n",
    "    \n",
    "    def column_filter():\n",
    "    \n",
    "        snli['snli_train'].drop(['annotator_labels', 'captionID', 'pairID', 'sentence1_binary_parse', 'sentence1_parse',\n",
    "                                'sentence2_binary_parse', 'sentence2_parse' ], axis=1, inplace=True),\n",
    "        mnli['multinli_train'].drop(['annotator_labels', 'genre', 'pairID', 'promptID', 'sentence1_binary_parse', \n",
    "                                    'sentence1_parse', 'sentence2_binary_parse', 'sentence2_parse'], axis=1, inplace=True)\n",
    "        \n",
    "        for round_str, sets in anli1.items():\n",
    "            anli1[round_str]['train'].drop(['uid', 'model_label', 'emturk', 'genre', 'reason', 'tag'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        cnli['chaosNLI_snli'].drop(['uid', 'label_dist', 'old_labels', 'old_label', 'source'], axis=1, inplace=True),\n",
    "\n",
    "        cnli['chaosNLI_mnli_m'].drop(['uid', 'label_dist', 'old_labels', 'old_label', 'source'], axis=1, inplace=True),\n",
    "\n",
    "        nli_var.drop(['task', 'original-dataset-label', 'id', 'num-NA'], axis=1, inplace=True)\n",
    "    \n",
    "    #anli2 = get_df_dict('anli_reanalyzed')\n",
    "\n",
    "    print('Getting SNLI dataframe...')\n",
    "    snli = get_df_dict('snli')\n",
    "    snli['snli_train'].drop(snli['snli_train'].loc[snli['snli_train']['label']=='-'].index, inplace=True)\n",
    "\n",
    "    print('Getting MNLI dataframe...')\n",
    "    mnli = get_df_dict('multinli')\n",
    "\n",
    "    print('Getting ANLI dataframes...')\n",
    "    anli1 = get_df_dict('anli')\n",
    "\n",
    "    \n",
    "    print('Getting CNLI dataframes...')\n",
    "    cnli = get_df_dict('chaosNLI')\n",
    "\n",
    "    print('Getting NLI variation dataframe...')\n",
    "    nli_var = pd.read_json(path_or_buf='./data/NLI_variation/NLI_variation_data.jsonl', lines=True)\n",
    "    #nli_var = normalise_headers(nli_var)\n",
    "\n",
    "    duplicate_snli_indices = get_NLIvar_duplicate_indices('snli')\n",
    "    duplicate_mnli_indices = get_NLIvar_duplicate_indices('mnli')\n",
    "    #print(len(snli['snli_train']))\n",
    "    snli['snli_train'].drop(index=duplicate_snli_indices, inplace=True)\n",
    "    snli['snli_train'].reset_index(drop=True, inplace=True)\n",
    "    #print(len(snli['snli_train']))\n",
    "\n",
    "    #print(len(mnli['multinli_train']))\n",
    "    mnli['multinli_train'].drop(index=duplicate_mnli_indices, inplace=True)\n",
    "    mnli['multinli_train'].reset_index(drop=True, inplace=True)\n",
    "    #print(len(mnli['multinli_train']))\n",
    "    \n",
    "    column_filter()\n",
    "    \n",
    "    print('Transforming SNLI data...')\n",
    "    snli_train_dataset = NLIData(snli['snli_train'])\n",
    "    \n",
    "    print('Transforming MNLI data...')\n",
    "    mnli_train_dataset = NLIData(mnli['multinli_train'])\n",
    "\n",
    "\n",
    "    print('Transforming ANLI data...')\n",
    "    \n",
    "    anli_training_datasets = {}\n",
    "    for round_str in anli1.keys():\n",
    "        print('\\t', round_str+'...')\n",
    "        anli_training_datasets[round_str] = NLIData(anli1[round_str]['train'])\n",
    "    \n",
    "    print('Transforming CNLI data...')\n",
    "\n",
    "    cnli_eval_datasets = {}\n",
    "    for name, df in cnli.items():\n",
    "        print('\\t', name+'...')\n",
    "        cnli_eval_datasets[name] = NLIData(df)\n",
    "\n",
    "    print('Transforming NLI variation data...')\n",
    "\n",
    "    NLI_var_eval_dataset = NLIData(nli_var, require_label=False)\n",
    "\n",
    "    return  snli_train_dataset, mnli_train_dataset, anli_training_datasets, cnli_eval_datasets, NLI_var_eval_dataset\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_encoding': {'input_ids': tensor([ 101, 2017, 2215, 2000, 8595, 1996, 6462, 1998, 2175,  102, 2017, 2123,\n",
       "          1005, 1056, 2215, 2000, 5245, 1996, 6462, 8217, 1010, 2021, 2738, 8595,\n",
       "          2009, 2524, 1012,  102]),\n",
       "  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1])},\n",
       " 'premise': 'you want to punch the button and go',\n",
       " 'hypothesis': \"You don't want to push the button lightly, but rather punch it hard.\",\n",
       " 'input_length': 28,\n",
       " 'unique_data': label_counter    {'e': 48, 'n': 45, 'c': 7}\n",
       " label_count                     [48, 45, 7]\n",
       " entropy                            1.295225\n",
       " Name: 1, dtype: object,\n",
       " 'label': 'e',\n",
       " 'label_tensor': [0]}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnli_eval_datasets['chaosNLI_mnli_m'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evalsets():\n",
    "    def column_filter():\n",
    "        cnli['chaosNLI_snli'].drop(['uid', 'label_dist', 'old_labels', 'old_label', 'source'], axis=1, inplace=True),\n",
    "\n",
    "        cnli['chaosNLI_mnli_m'].drop(['uid', 'label_dist', 'old_labels', 'old_label', 'source'], axis=1, inplace=True),\n",
    "\n",
    "        nli_var.drop(['task', 'original-dataset-label', 'id', 'num-NA'], axis=1, inplace=True)\n",
    "    \n",
    "    print('Getting CNLI dataframes...')\n",
    "\n",
    "\n",
    "\n",
    "    cnli = get_df_dict('chaosNLI')\n",
    "\n",
    "    print('Getting NLI variation dataframe...')\n",
    "    nli_var = pd.read_json(path_or_buf='./data/NLI_variation/NLI_variation_data.jsonl', lines=True)\n",
    "\n",
    "    column_filter()\n",
    "\n",
    "    print('Transforming CNLI data...')\n",
    "    cnli_eval_datasets = {}\n",
    "    for name, df in cnli.items():\n",
    "        print('\\t', name+'...')\n",
    "        cnli_eval_datasets[name] = NLIData(df)\n",
    "    \n",
    "    print('Transforming NLI variation data...')\n",
    "    \n",
    "    NLI_var_eval_dataset = NLIData(nli_var, require_label=False)\n",
    "\n",
    "    return cnli_eval_datasets, NLI_var_eval_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_pred, y_test, classes, logfile = False):\n",
    "    # vestigial function, used to check mid-training if the model seemed to make any progress\n",
    "    # retired after I implemented mid-training plots for the same purpose\n",
    "    \n",
    "    \"\"\"\n",
    "    Prints accuracy and macro f-score based on inputs.\n",
    "    Writes to file instead if logfile == truthy\n",
    "    Args:\n",
    "        y_pred: list of class predictions made by model\n",
    "        y_test: list of gold labels of evaluation set where y_test[i] corresponds to y_pred[i]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if logfile:\n",
    "        print(logfile)\n",
    "        print('Logging results to file instead of printing: see model directory')\n",
    "        resultspath = os.path.join(os.getcwd(), logfile)\n",
    "        print(resultspath)\n",
    "        log = open(resultspath, 'a')\n",
    "        sys.stdout = log\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #p_scores = precision_score(y_test, y_pred, average = None, zero_division=0.0)\n",
    "    #r_scores = recall_score(y_test, y_pred, average = None, zero_division=0.0)\n",
    "    f_scores = f1_score(y_test, y_pred, average = None, zero_division=0.0)\n",
    "    #macro_p = precision_score(y_test, y_pred, average = \"macro\", zero_division=0.0)\n",
    "    #macro_r = recall_score(y_test, y_pred, average = \"macro\", zero_division=0.0)\n",
    "    macro_f = f1_score(y_test, y_pred, average = \"macro\", zero_division=0.0)\n",
    "\n",
    "    print()\n",
    "    print(\"accuracy is\", accuracy)\n",
    "    print()\n",
    "\n",
    "    for label, f_score in zip(classes, f_scores):\n",
    "        print(\"f-score for label '{}' is {}\".format(label, f_score))\n",
    "    print(\"macro f-score is\", macro_f)\n",
    "    print('------------------------------------------------')\n",
    "    if logfile:\n",
    "        log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ent = cnli_eval_datasets['chaosNLI_mnli_m'][0]['unique_data']['entropy']\n",
    "#lc = cnli_eval_datasets['chaosNLI_mnli_m'][0]['unique_data']['label_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = model_specifier['sequence_classification'].from_pretrained(model_specifier['model_name'], num_labels=3).to(device)\n",
    "model(batch['input_ids'],\n",
    "                                attention_mask=batch['attention_masks'],\n",
    "                                token_type_ids=batch['token_type_ids'],\n",
    "                                labels=batch['label_tensors'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_testing(dataset, modelpath):\n",
    "    # just some work I did for myself to ensure I got the entropy right, figured I'd leave it in\n",
    "\n",
    "    loader = DataLoader(dataset, collate_fn=collate_fn, batch_size=1)\n",
    "    #modelpath = './models/toy-models/r2/e2'\n",
    "    model = model_specifier['sequence_classification'].from_pretrained(modelpath).to(device)\n",
    "    tokenizer = model_specifier['tokenizer'].from_pretrained(modelpath)\n",
    "    #model.load_state_dict(torch.load(modelpath, weights_only=True))\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in loader:\n",
    "        outputs = model(batch['input_ids'].to(device),\n",
    "                        attention_mask=batch['attention_masks'].to(device),\n",
    "                        token_type_ids=batch['token_type_ids'].to(device),\n",
    "                    )\n",
    "        \n",
    "        # cnli includes entropy with base2 logarithm\n",
    "        # example:\n",
    "        #cnli_ent = cnli_eval_datasets['chaosNLI_mnli_m'][0]['unique_data']['entropy']\n",
    "        print()\n",
    "        print(type(batch))\n",
    "        #print(batch['unique_data'][0]['entropy'])\n",
    "        cnli_ent = batch['unique_data'][0]['entropy']\n",
    "        print('cnli entropy, base2:',cnli_ent)\n",
    "        print()\n",
    "\n",
    "        # label count of the same example:\n",
    "        #lc = cnli_eval_datasets['chaosNLI_mnli_m'][0]['unique_data']['label_count']\n",
    "        lc = batch['unique_data'][0]['label_count']\n",
    "\n",
    "        #from count to label % distribution:\n",
    "        dist = [count/100 for count in lc]\n",
    "\n",
    "        # DIY numpy entropy from dist\n",
    "        p = np.array(dist)\n",
    "        logp = np.log2(p)\n",
    "        numpy_ent1 = np.sum(-p*logp)\n",
    "\n",
    "        # numpy entropy from lc\n",
    "        p = np.array(lc)\n",
    "        logp = np.log2(p)\n",
    "        numpy_ent2 = np.sum(-p*logp)\n",
    "\n",
    "        print('numpy:')\n",
    "        print('np log2 entropy from dist:', numpy_ent1)\n",
    "        print('np log2 entropy from count:', numpy_ent2)\n",
    "        print()\n",
    "\n",
    "        p_tensor_dist = torch.Tensor(dist)\n",
    "        tensor_lc = torch.Tensor(lc)\n",
    "\n",
    "        pt_ent1 = Categorical(probs = p_tensor_dist).entropy()\n",
    "        pt_ent2 = Categorical(probs = tensor_lc).entropy()\n",
    "\n",
    "        print('pt, default=natlog:')\n",
    "        print('pt ent from dist:', pt_ent1)\n",
    "        print('pt ent from count:', pt_ent2)\n",
    "        print()\n",
    "\n",
    "        sp_ent_dist = entropy(dist)\n",
    "        sp_ent_lc = entropy(lc)\n",
    "        sp_ent_dist_b2 = entropy(dist, base=2)\n",
    "\n",
    "        print('scipy, default=natlog')\n",
    "        print('scipy entropy from dist:', sp_ent_dist)\n",
    "        print('scipy entropy from count:', sp_ent_lc)\n",
    "        print('scipy entropy from dist base2:', sp_ent_dist_b2)\n",
    "        print()\n",
    "\n",
    "        #entropy = batch['unique_data']['entropy']\n",
    "        # torch.softmax(scores, -1).squeeze()\n",
    "        # probs = torch.softmax(scores, -1).squeeze()        \n",
    "\n",
    "        logits = outputs[0][0]\n",
    "        probs = torch.softmax(logits, -1).squeeze()\n",
    "        #probs2 = torch.softmax(logits, -1).squeeze().cpu().detach().numpy()\n",
    "        #probs2 = probs = torch.softmax(logits, -1)\n",
    "        #print(logits)\n",
    "        #print(probs)\n",
    "        #print(probs2)\n",
    "        #pt_ent_from_logits_CB = ContinuousBernoulli(logits = p_tensor_dist).entropy()\n",
    "        #pt_ent_from_softmax_CB = ContinuousBernoulli(probs = p_tensor_dist).entropy()\n",
    "        print('logits',logits)\n",
    "        print('probs',probs)\n",
    "        pt_ent_from_logits = Categorical(logits = p_tensor_dist).entropy()\n",
    "        pt_ent_from_softmax = Categorical(probs = p_tensor_dist).entropy()\n",
    "        np_probs = probs.cpu().detach().numpy()\n",
    "        print('np_probs',np_probs)\n",
    "        scipy_ent_from_logits = entropy(logits.cpu().detach().numpy())\n",
    "        scipy_ent_from_softmax = entropy(np_probs)\n",
    "        scipy_ent_from_logits_b2 = entropy(logits.cpu().detach().numpy(), base=2)\n",
    "        scipy_ent_from_softmax_b2 = entropy(np_probs, base=2)\n",
    "        #scipy_ent_from_softmax2_b2 = entropy(probs2, base=2)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        p = np.array(np_probs)\n",
    "        log2p = np.log2(p)\n",
    "        logp = np.log(p) #nat log\n",
    "        model_numpy_entb2 = np.sum(-p*log2p)\n",
    "        model_numpy_natent = np.sum(-p*logp)\n",
    "\n",
    "        #print('pt entropy from logits, CB:', pt_ent_from_logits_CB)\n",
    "        #print('pt entropy from softmax, CB:', pt_ent_from_softmax_CB)\n",
    "        print('pt entropy from logits:', pt_ent_from_logits)\n",
    "        print('pt entropy from softmax:', pt_ent_from_softmax)\n",
    "        print('scipy entropy from logits:', scipy_ent_from_logits)\n",
    "        print('scipy entropy from softmax:', scipy_ent_from_softmax)\n",
    "        print('scipy entropy from logits, b2:', scipy_ent_from_logits_b2)\n",
    "        print('scipy entropy from softmax, b2:', scipy_ent_from_softmax_b2)\n",
    "        #print('scipy entropy from softmax2:', scipy_ent_from_softmax2_b2)\n",
    "        print()\n",
    "        print('model numpy entropy, natlog', model_numpy_natent)\n",
    "        print('model numpy entropy, b2', model_numpy_entb2)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_example, _ = random_split(cnli_eval_datasets['chaosNLI_mnli_m'], [1, len(cnli_eval_datasets['chaosNLI_mnli_m'])-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just some work I did for myself to ensure I got the entropy right, figured I'd leave it in\n",
    "\n",
    "# cnli includes entropy with base2 logarithm\n",
    "# from example 0 of cnli_mnli:\n",
    "cnli_ent = cnli_eval_datasets['chaosNLI_mnli_m'][0]['unique_data']['entropy']\n",
    "print('cnli entropy, base2:',cnli_ent)\n",
    "print()\n",
    "\n",
    "# label count of the same:\n",
    "lc = cnli_eval_datasets['chaosNLI_mnli_m'][0]['unique_data']['label_count']\n",
    "\n",
    "#from count to label % distribution:\n",
    "dist = [count/100 for count in lc]\n",
    "\n",
    "# DIY numpy entropy from dist\n",
    "p = np.array(dist)\n",
    "logp = np.log2(p)\n",
    "numpy_ent1 = np.sum(-p*logp)\n",
    "\n",
    "# numpy entropy from lc\n",
    "p = np.array(lc)\n",
    "logp = np.log2(p)\n",
    "numpy_ent2 = np.sum(-p*logp)\n",
    "\n",
    "print('numpy:')\n",
    "print('np log2 entropy from dist:', numpy_ent1)\n",
    "print('np log2 entropy from count:', numpy_ent2)\n",
    "\n",
    "p_tensor_dist = torch.Tensor(dist)\n",
    "tensor_lc = torch.Tensor(lc)\n",
    "\n",
    "pt_ent1 = Categorical(probs = p_tensor_dist).entropy()\n",
    "pt_ent2 = Categorical(probs = tensor_lc).entropy()\n",
    "\n",
    "print('pt, default=natlog:')\n",
    "print('pt ent from dist:', pt_ent1)\n",
    "print('pt ent from count:', pt_ent2)\n",
    "print()\n",
    "\n",
    "sp_ent_dist = entropy(dist)\n",
    "sp_ent_lc = entropy(lc)\n",
    "sp_ent_dist_b2 = entropy(dist, base=2)\n",
    "\n",
    "print('scipy, default=natlog')\n",
    "print('scipy entropy from dist:', sp_ent_dist)\n",
    "print('scipy entropy from count:', sp_ent_lc)\n",
    "print('scipy entropy from dist base2:', sp_ent_dist_b2)\n",
    "\n",
    "#dist = torch.softmax(count, -1)\n",
    "#dist = torch.softmax(dist, -1).cpu()\n",
    "#print(dist)\n",
    "\n",
    "#lc = torch.as_tensor(lc, dtype=float)\n",
    "#lc = torch.softmax(lc, -1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting CNLI dataframes...\n",
      "Getting NLI variation dataframe...\n",
      "Transforming CNLI data...\n",
      "\t chaosNLI_snli...\n",
      "\t chaosNLI_mnli_m...\n",
      "Transforming NLI variation data...\n"
     ]
    }
   ],
   "source": [
    "cnli_eval_datasets, NLI_var_eval_dataset = get_evalsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_encoding': {'input_ids': tensor([  101,  8529,  1011, 14910,  8529,  1011, 14910,  3398,  2092,  7910,\n",
       "           1045,  2064,  2156,  2017,  2113,  2009,  1005,  1055,  2009,  1005,\n",
       "           1055,  2009,  1005,  1055,  2009,  1005,  1055,  2785,  1997,  6057,\n",
       "           2138,  2057,  2009,  3849,  2066,  2057,  5414,  2769,  2017,  2113,\n",
       "           2057,  2769,  2007,  7817,  4987,  1998,  2065,  1996,  2231,  3431,\n",
       "           1998,  1996,  2406,  2008,  2057,  5414,  1996,  2769,  2000,  8529,\n",
       "           1045,  2064,  2156,  2339,  1996,  2453,  2031,  1037,  2367,  7729,\n",
       "           2875,  7079,  2009,  2067,  2009,  1005,  1055,  1037,  2843,  2149,\n",
       "           2008,  2017,  2113,  2057,  2123,  1005,  1056,  2428,  5414,  2769,\n",
       "           2000,  2000,  3032,  2057,  5414,  2769,  2000,  6867,  1998,  2009,\n",
       "           1005,  1055,  1996,   102,  2057,  2123,  1005,  1056,  5414,  1037,\n",
       "           2843,  1997,  2769,  1012,   102]),\n",
       "  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])},\n",
       " 'premise': \"um-hum um-hum yeah well uh i can see you know it's it's it's it's kind of funny because we it seems like we loan money you know we money with strings attached and if the government changes and the country that we loan the money to um i can see why the might have a different attitude towards paying it back it's a lot us that  you know we don't really loan money to to countries we loan money to governments and it's the\",\n",
       " 'hypothesis': \"We don't loan a lot of money.\",\n",
       " 'input_length': 115,\n",
       " 'unique_data': label_counter    {'c': 20, 'e': 12, 'n': 68}\n",
       " label_count                     [12, 68, 20]\n",
       " entropy                               1.2098\n",
       " Name: 0, dtype: object,\n",
       " 'label': 'n',\n",
       " 'label_tensor': [1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnli_eval_datasets['chaosNLI_mnli_m'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = DataLoader(cnli_eval_datasets['chaosNLI_snli'], collate_fn=collate_fn, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02, 0.03, 0.05, 0.06])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [2,3,5,6]\n",
    "arr = np.array(lst)\n",
    "arr=arr/100\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(unique_data, logits):\n",
    "    '''\n",
    "    Extracts label distribution from evaluation set data and computes entropy\n",
    "\n",
    "    Arg:\n",
    "        unique_data: unbatched \n",
    "    '''\n",
    "    if 'label_count' in unique_data:\n",
    "        hum_dist = [c/100 for c in unique_data['label_count']]\n",
    "    else:\n",
    "        #print(unique_data)\n",
    "        hum_dist = [0, 0, 0]\n",
    "        lb_scales = unique_data['labels']\n",
    "\n",
    "        # Discretize the labels to make them interface in the model, by the same thresholds as\n",
    "        # in the original paper (Pavlick, Kwiatkowski, 2019).\n",
    "        # This means that while I don't make use of the grading scale, I still utilise the annotator variation.\n",
    "        # Also makes the NLI_val results more comparable to those from CNLI.\n",
    "        for l in lb_scales:\n",
    "            if l > 16.7:\n",
    "                hum_dist[0] = hum_dist[0]+1\n",
    "            elif l < -16.7:\n",
    "                hum_dist[2] = hum_dist[2]+1\n",
    "            else:\n",
    "                hum_dist[1] = hum_dist[1]+1\n",
    "        hum_dist = [c/sum(hum_dist) for c in hum_dist]\n",
    "    \n",
    "    hum_ent = entropy(hum_dist, base=2)\n",
    "\n",
    "    model_dist = torch.softmax(logits, -1).squeeze().cpu().detach().numpy()\n",
    "    model_ent = entropy(model_dist, base=2)  \n",
    "    return hum_ent, model_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/496 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels           [49.999903547520425, -49.99995111441214, -49.9...\n",
      "normed-labels    [-1.049158911374859, -2.056001753990448, -1.38...\n",
      "Name: 0, dtype: object\n",
      "xnorm: 4.380989543882038\n",
      "1.3704165065672542 0.648357739453004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [], [], [], [])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(modelpath='./models/bert-base-uncased-bs32-eps6-lr5e-05/r4/e6', device=device, eval_dataset=NLI_var_eval_dataset) #eval_dataset=NLI_var_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(eval_dataset, modelpath=False, modelin= False, device='cpu', toy_run=toy_run):\n",
    "    #print(eval_dataset[0])\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "\n",
    "    if not modelpath and not modelin:\n",
    "        print(\"Suppy either 'modelpath' or 'modelin' argument\")\n",
    "        return None    \n",
    "\n",
    "    loader = DataLoader(dataset=eval_dataset, collate_fn=collate_fn, batch_size=1)\n",
    "    if modelpath:\n",
    "        model = model_specifier['sequence_classification'].from_pretrained(modelpath).to(device)\n",
    "    if modelin:\n",
    "        model = modelin\n",
    "\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    human_entropies = []\n",
    "    model_entropies = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in enumerate(tqdm.tqdm(loader)):\n",
    "            if toy_run:\n",
    "                if len(human_entropies) > 10:\n",
    "                    break\n",
    "            if 'label_tensors' in batch:\n",
    "                label_input = batch['label_tensors'].to(device)\n",
    "            else:\n",
    "                label_input = None # no gold labels in NLI_var\n",
    "\n",
    "            outputs = model(batch['input_ids'].to(device),\n",
    "                                attention_mask=batch['attention_masks'].to(device),\n",
    "                                token_type_ids=batch['token_type_ids'].to(device),\n",
    "                                labels=label_input)\n",
    "            \n",
    "            \n",
    "            if 'label_tensors' in batch:\n",
    "                loss, logits = outputs[:2]\n",
    "                pred = torch.argmax(logits)\n",
    "                gold = batch['label_tensors'][0][0]\n",
    "                y_test.append(gold.cpu())\n",
    "                y_pred.append(pred.cpu())\n",
    "            else:\n",
    "                logits = outputs[0]\n",
    "\n",
    "\n",
    "            hum_ent, model_ent = get_entropy(batch['unique_data'][0], logits)\n",
    "            human_entropies.append(hum_ent)\n",
    "            model_entropies.append(model_ent)\n",
    "    \n",
    "        return y_pred, y_test, human_entropies, model_entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_id,test_log, test_oe, test_e, test_out, y_test, y_pred, collected_logits, collected_entropies = test(modelpath=False, modelin= model, device=device, eval_dataset=cnli_eval_datasets['chaosNLI_mnli_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NLI_var_eval_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mNLI_var_eval_dataset\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_data\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormed-labels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NLI_var_eval_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "NLI_var_eval_dataset[0]['unique_data']['normed-labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = './models/bert-base-uncased-bs32-eps6-lr5e-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suppy either 'modelpath' or 'modelin' argument\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred, y_test, collected_entropies, collected_prior_entropies, unique_data \u001b[38;5;241m=\u001b[39m test(modelpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice, eval_dataset\u001b[38;5;241m=\u001b[39mcnli_eval_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchaosNLI_mnli_m\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "y_pred, y_test, collected_entropies, collected_prior_entropies, unique_data = test(modelpath=False, device=device, eval_dataset=cnli_eval_datasets['chaosNLI_mnli_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hyperparameters.json', 'r1', 'r2', 'r3', 'r4']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(modeldir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(d, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud = cnli_eval_datasets['chaosNLI_mnli_m'][0]['unique_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [c/100 for c in ud['label_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.209800338660482"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ud['entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2098003386604825"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(d, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_ent_dist = entropy(dist)\n",
    "        sp_ent_lc = entropy(lc)\n",
    "        sp_ent_dist_b2 = entropy(dist, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12, 0.68, 0.2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sets = {'ChaosNLI-MNLI': cnli_eval_datasets['chaosNLI_mnli_m'], 'ChaosNLI-SNLI': cnli_eval_datasets['chaosNLI_snli'], 'NLIVariation': NLI_var_eval_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical(data_dict):\n",
    "    x = data_dict['x']\n",
    "    if 'type' in data_dict:\n",
    "        plotfunc = data_dict['type']\n",
    "    else:\n",
    "        plotfunc = plt.plot\n",
    "    for y, label in zip(data_dict['y'], data_dict['legend_lables']):\n",
    "        print(y)\n",
    "        print(x)\n",
    "        #names = list(line_data.keys())\n",
    "        #values = list(line_data.values())\n",
    "        # y is list of numerical data\n",
    "        # data_dict['y'] is a list of y, with each element = a graph line\n",
    "        # x is list of epochs (xticks)\n",
    "        # label is legend name\n",
    "        #if plotfunc == plt.plot:\n",
    "        #    plotfunc(x, y, label=label, marker = 'o')\n",
    "        #else:\n",
    "        #    plotfunc(x, y, label=label)\n",
    "        plt.plot(x, y, label=label, marker = 'o')\n",
    "    plt.legend()\n",
    "    plt.title(data_dict['title'], fontsize=16)\n",
    "    plt.savefig(data_dict['filepath'], bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_n_data(all_sets_data, n):\n",
    "    epoch_n_data = [setdata[n] for setdata in all_sets_data]\n",
    "    return epoch_n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_sublists_by_idx(list_of_lists):\n",
    "    print(list_of_lists)\n",
    "    \n",
    "    reorganised = []\n",
    "    for i in range(len(list_of_lists[0])):\n",
    "        reorganised.append([item[i] for item in list_of_lists])\n",
    "    print(reorganised)\n",
    "    return reorganised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n",
      "[[1, 4], [2, 5], [3, 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 4], [2, 5], [3, 6]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reorder_sublists_by_idx([[1,2,3], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_eval(model_specifier, modeldir, eval_sets):\n",
    "    \n",
    "    plot_data_dicts = []\n",
    "    overall_resultspath = os.path.join(modeldir, 'results')\n",
    "    if not os.path.exists(overall_resultspath):\n",
    "        os.mkdir(overall_resultspath)\n",
    "    overall_results = {}\n",
    "    #for eval_set_name, eval_set in list(eval_sets.items()):\n",
    "    final_epoch_accuracies = []\n",
    "    final_epoch_mfscores = []\n",
    "    final_epoch_ent_corrs = []\n",
    "    rounds = []\n",
    "    testcount = 1\n",
    "    print('Running evaluation for every epoch-checkpoint of every round on each set.')\n",
    "    for roundname in os.listdir(modeldir):\n",
    "        if 'json' in roundname or 'results' in roundname:\n",
    "            continue\n",
    "        rounds.append(roundname)\n",
    "        rounddir = os.path.join(modeldir, roundname)\n",
    "        round_resultspath = os.path.join(rounddir, 'results')\n",
    "        if not os.path.exists(round_resultspath):\n",
    "            os.mkdir(round_resultspath)\n",
    "        #accuracy_per_epoch = []\n",
    "        #mf_scores_per_epoch = []\n",
    "        #entropy_correlation_per_epoch = []\n",
    "        epochs = []\n",
    "        per_set_accs = []\n",
    "        per_set_mfs = []\n",
    "        per_set_entcorrs = []\n",
    "        for eval_set_name, eval_set in list(eval_sets.items()):\n",
    "            #if 'results' in epoch:\n",
    "            #    continue            \n",
    "            per_epoch_accs =  []\n",
    "            per_epoch_mfs = []\n",
    "            per_epoch_entcorrs = []\n",
    "            #setnames = []\n",
    "            #print(len(os.listdir(rounddir)))\n",
    "            #print(os.listdir(rounddir))\n",
    "       \n",
    "            for epoch in os.listdir(rounddir):\n",
    "                epochdir = os.path.join(rounddir, epoch)\n",
    "                if 'results' in epoch: # checks if the current iteration is over a results directory at round the round level (i.e sibling of 'e1' etc.)\n",
    "                                        # not to be mistaken with the next few lines of code\n",
    "                                        # which creates a results directory for epoch-level results.\n",
    "                    continue\n",
    "\n",
    "                epoch_resultspath = os.path.join(epochdir, 'results')\n",
    "                if not os.path.exists(epoch_resultspath):\n",
    "                    os.mkdir(epoch_resultspath)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                model =  model_specifier['sequence_classification'].from_pretrained(epochdir).to(device)\n",
    "                if len(epochs) < len(os.listdir(rounddir))-1: # Number of checkpoints per round\n",
    "                                                # could have just hardcoded 6, since that's the number of epochs I train with,\n",
    "                                                # but I guess this is technically more flexible  \n",
    "                    epochs.append(epoch)\n",
    "                #setnames.append(eval_set_name)\n",
    "\n",
    "                nrounds = len(os.listdir(modeldir))-2\n",
    "                nepochs = len(rounddir)-1\n",
    "                testruns = len(eval_sets)*nrounds*nepochs\n",
    "                print('Evaluation {} out of {}'.format(testcount, testruns))\n",
    "                print('Getting scores for {} {} on {}.'.format(roundname, epoch, eval_set_name))\n",
    "                y_pred, y_test, human_entropies, model_entropies = test(modelin=model, device=device, eval_dataset=eval_set)\n",
    "                human_entropies = np.array(human_entropies)\n",
    "                model_entropies = np.array(model_entropies)\n",
    "                pearson_c = np.corrcoef(model_entropies, human_entropies)[0][1]\n",
    "                r, p = pearsonr(model_entropies, human_entropies)\n",
    "                #print(r, pearson_c)\n",
    "                #print(p)\n",
    "                \n",
    "                \n",
    "                per_epoch_entcorrs.append(pearson_c)\n",
    "                if not eval_set_name == 'NLIVariation':\n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    #accuracy_per_epoch.append(accuracy)\n",
    "                    macro_f = f1_score(y_test, y_pred, average = \"macro\", zero_division=0.0)\n",
    "                    per_epoch_accs.append(accuracy)\n",
    "                    per_epoch_mfs.append(macro_f)\n",
    "\n",
    "                plt.scatter(human_entropies, model_entropies)\n",
    "\n",
    "                #a, b = np.polyfit(x, y, 1)        \n",
    "                # from https://pythonguides.com/matplotlib-best-fit-line/ and https://www.statology.org/line-of-best-fit-python/\n",
    "                #y_line = theta[1] + theta[0] * np.array(human_entropies)\n",
    "                #plt.plot(model_entropies, y_line, 'r')\n",
    "                a, b = np.polyfit(human_entropies, model_entropies, 1)\n",
    "                plt.plot(human_entropies, a*human_entropies+b, color='orange') \n",
    "                plt.title(\" \".join((eval_set_name, roundname, epoch, 'entropy with line of best fit')))\n",
    "                plt.xlabel('Human entropy')\n",
    "                plt.ylabel('Model entropy')\n",
    "                plt.annotate('r = {:.2f}, p = {:.2f}'.format(r, p), xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "                plt.savefig(os.path.join(epoch_resultspath, eval_set_name+'-entropies'), bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "                #mf_scores_per_epoch.append(macro_f)\n",
    "\n",
    "                '''if epoch == 'e6':\n",
    "                    final_epoch_accuracies.append(accuracy)\n",
    "                    final_epoch_mfscores.append(macro_f)\n",
    "                    final_epoch_ent_corrs.append(pearson_c)'''\n",
    "                \n",
    "            if not eval_set_name == 'NLIVariation':               \n",
    "                per_set_accs.append(per_epoch_accs)\n",
    "                per_set_mfs.append(per_epoch_mfs)\n",
    "            per_set_entcorrs.append(per_epoch_entcorrs)\n",
    "    \n",
    "        \n",
    "\n",
    "        plot_input_accuracy = {'y': per_set_accs, 'x': epochs, 'legend_lables': list(eval_sets.keys())[:2],\n",
    "                               'title': roundname+' Accuracy', 'filepath': os.path.join(round_resultspath, 'accuracy')}\n",
    "        plot_input_mf = {'y': per_set_mfs, 'x': epochs, 'legend_lables': list(eval_sets.keys())[:2],\n",
    "                               'title': roundname+' Macro F1', 'filepath': os.path.join(round_resultspath, 'macrof1')}\n",
    "        plot_input_ent = {'y': per_set_entcorrs, 'x': epochs, 'legend_lables': list(eval_sets.keys()),\n",
    "                               'title': roundname+' Model/human entropy correlation', 'filepath': os.path.join(round_resultspath, 'entropy_correlation')}\n",
    "\n",
    "        \n",
    "        # Get metrics from last epoch for each set:\n",
    "        # I don't do this here, but this could instead fetch the result of a specified epoch,\n",
    "        # e.g., if I wanted to get specifically the best epoch\n",
    "        final_epoch_accs = epoch_n_data(per_set_accs, -1)\n",
    "        final_epoch_mfs = epoch_n_data(per_set_mfs, -1)\n",
    "        final_epoch_ent = epoch_n_data(per_set_entcorrs, -1)\n",
    "\n",
    "        final_epoch_accuracies.append(final_epoch_accs)\n",
    "        final_epoch_mfscores.append(final_epoch_mfs)\n",
    "        final_epoch_ent_corrs.append(final_epoch_ent)\n",
    "    \n",
    "    # final_epoch_acc/mf/ents are lists of lists whose outer elements are per round data and subelements of those elements are per test set\n",
    "    # data for the respective round. I want to plot metrics across rounds (so round number on X), and plot_categorical expects y to be\n",
    "    # list of lists where the INNER list corresponds to the x-ticks. So I invert:\n",
    "    # (could have avoided this by iterating over sets before over rounds, but my per-round plotting above has a different preference)\n",
    "\n",
    "    reorganised_acc = reorder_sublists_by_idx(final_epoch_accuracies)\n",
    "    reorganised_mf = reorder_sublists_by_idx(final_epoch_mfscores)\n",
    "    reorganised_ent = reorder_sublists_by_idx(final_epoch_ent_corrs)\n",
    "\n",
    "\n",
    "    \n",
    "    plot_input_accuracy = {'y': reorganised_acc, 'x': rounds, 'legend_lables': list(eval_sets.keys())[:2],\n",
    "                        'title': 'Final epoch accuracy across rounds', 'filepath': os.path.join(overall_resultspath, 'accuracy'), 'type': plt.bar}\n",
    "    plot_input_mf = {'y': reorganised_mf, 'x': rounds, 'legend_lables': list(eval_sets.keys())[:2],\n",
    "                        'title': 'Final epoch macro F1 across rounds', 'filepath': os.path.join(overall_resultspath, 'macrof1'), 'type': plt.bar}\n",
    "    plot_input_ent = {'y': reorganised_ent, 'x': rounds, 'legend_lables': list(eval_sets.keys()),\n",
    "                        'title': ' Final epoch model/human entropy correlation across rounds', 'filepath': os.path.join(overall_resultspath, 'entropy_correlation'),\n",
    "                        'type': plt.bar}\n",
    "\n",
    "    plot_data_dicts.extend([plot_input_accuracy, plot_input_mf, plot_input_ent])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    for plot_data in plot_data_dicts:\n",
    "        print(plot_data)\n",
    "        plot_categorical(plot_data)\n",
    "        \n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "                        #p_scores = precision_score(y_test, y_pred, average = None, zero_division=0.0)\n",
    "                        #r_scores = recall_score(y_test, y_pred, average = None, zero_division=0.0)\n",
    "                        #f_scores = f1_score(y_test, y_pred, average = None, zero_division=0.0)\n",
    "                        #macro_p = precision_score(y_test, y_pred, average = \"macro\", zero_division=0.0)\n",
    "                        #macro_r = recall_score(y_test, y_pred, average = \"macro\", zero_division=0.0)\n",
    "                    \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'e1': 1, 'e2':2, 'e3': 2}\n",
    "data2 = {'e1':2, 'e2':2, 'e3': 5}\n",
    "datalist = [data1, data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = {'1': [1,2,3,4,3], '2': data2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'e1': 1, 'e2': 2, 'e3': 2}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalist[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3['1'].append(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': [1, 2, 3, 4, 3, 10], '2': {'e1': 2, 'e2': 2, 'e3': 5}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_categorical() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatalist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtesttitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: plot_categorical() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "plot_categorical(datalist, ['r1', 'r2'], 'testtitle',os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e1 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e2 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e3 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:18<00:00, 84.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e4 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e5 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 84.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e6 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 81.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e1 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e2 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e3 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e4 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e5 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e6 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e1 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 86.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e2 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e3 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e4 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e5 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r1 e6 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e1 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 82.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e2 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e3 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e4 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e5 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e6 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e1 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e2 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e3 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e4 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e5 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e6 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e1 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e2 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e3 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e4 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e5 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r2 e6 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e1 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e2 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e3 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e4 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e5 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e6 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e1 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 81.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e2 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e3 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e4 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e5 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e6 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e1 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 86.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e2 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e3 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e4 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e5 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r3 e6 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:05<00:00, 85.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e1 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e2 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e3 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e4 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e5 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e6 on ChaosNLI-MNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:19<00:00, 83.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e1 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e2 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e3 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:18<00:00, 83.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e4 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:20<00:00, 73.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e5 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:21<00:00, 69.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e6 on ChaosNLI-SNLI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1514/1514 [00:21<00:00, 69.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e1 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:06<00:00, 72.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e2 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:06<00:00, 72.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e3 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:06<00:00, 71.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e4 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:06<00:00, 71.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e5 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:06<00:00, 71.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting scores for r4 e6 on NLIVariation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 496/496 [00:06<00:00, 71.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5272045028142589, 0.6941875825627477], [0.4965603502188868, 0.6994715984147952], [0.48530331457160725, 0.6955085865257595], [0.5240775484677924, 0.5495376486129459]]\n",
      "[[0.5272045028142589, 0.4965603502188868, 0.48530331457160725, 0.5240775484677924], [0.6941875825627477, 0.6994715984147952, 0.6955085865257595, 0.5495376486129459]]\n",
      "[[0.5141128564738868, 0.6784889375770216], [0.48903402547019564, 0.6834244914047201], [0.4794599767472101, 0.6734750207351858], [0.5096427333253989, 0.512771292645598]]\n",
      "[[0.5141128564738868, 0.48903402547019564, 0.4794599767472101, 0.5096427333253989], [0.6784889375770216, 0.6834244914047201, 0.6734750207351858, 0.512771292645598]]\n",
      "[[0.013409190772003471, 0.25620178453112236, 0.10670535153856693], [0.04230379712101827, 0.23248622458546903, 0.09171723522793551], [0.037526260861654184, 0.23770724063120177, 0.022042792682386927], [0.014017172268292301, 0.12545435679807823, 0.09835242863559304]]\n",
      "[[0.013409190772003471, 0.04230379712101827, 0.037526260861654184, 0.014017172268292301], [0.25620178453112236, 0.23248622458546903, 0.23770724063120177, 0.12545435679807823], [0.10670535153856693, 0.09171723522793551, 0.022042792682386927, 0.09835242863559304]]\n",
      "{'y': [[0.5272045028142589, 0.4965603502188868, 0.48530331457160725, 0.5240775484677924], [0.6941875825627477, 0.6994715984147952, 0.6955085865257595, 0.5495376486129459]], 'x': ['r1', 'r2', 'r3', 'r4'], 'legend_lables': ['ChaosNLI-MNLI', 'ChaosNLI-SNLI'], 'title': 'Final epoch accuracy across rounds', 'filepath': './models/bert-base-uncased-bs32-eps6-lr5e-05/results/accuracy', 'type': <function bar at 0x7ff816166480>}\n",
      "[0.5272045028142589, 0.4965603502188868, 0.48530331457160725, 0.5240775484677924]\n",
      "['r1', 'r2', 'r3', 'r4']\n",
      "[0.6941875825627477, 0.6994715984147952, 0.6955085865257595, 0.5495376486129459]\n",
      "['r1', 'r2', 'r3', 'r4']\n",
      "{'y': [[0.5141128564738868, 0.48903402547019564, 0.4794599767472101, 0.5096427333253989], [0.6784889375770216, 0.6834244914047201, 0.6734750207351858, 0.512771292645598]], 'x': ['r1', 'r2', 'r3', 'r4'], 'legend_lables': ['ChaosNLI-MNLI', 'ChaosNLI-SNLI'], 'title': 'Final epoch macro F1 across rounds', 'filepath': './models/bert-base-uncased-bs32-eps6-lr5e-05/results/macrof1', 'type': <function bar at 0x7ff816166480>}\n",
      "[0.5141128564738868, 0.48903402547019564, 0.4794599767472101, 0.5096427333253989]\n",
      "['r1', 'r2', 'r3', 'r4']\n",
      "[0.6784889375770216, 0.6834244914047201, 0.6734750207351858, 0.512771292645598]\n",
      "['r1', 'r2', 'r3', 'r4']\n",
      "{'y': [[0.013409190772003471, 0.04230379712101827, 0.037526260861654184, 0.014017172268292301], [0.25620178453112236, 0.23248622458546903, 0.23770724063120177, 0.12545435679807823], [0.10670535153856693, 0.09171723522793551, 0.022042792682386927, 0.09835242863559304]], 'x': ['r1', 'r2', 'r3', 'r4'], 'legend_lables': ['ChaosNLI-MNLI', 'ChaosNLI-SNLI', 'NLIVariation'], 'title': ' Final epoch model/human entropy correlation across rounds', 'filepath': './models/bert-base-uncased-bs32-eps6-lr5e-05/results/entropy_correlation', 'type': <function bar at 0x7ff816166480>}\n",
      "[0.013409190772003471, 0.04230379712101827, 0.037526260861654184, 0.014017172268292301]\n",
      "['r1', 'r2', 'r3', 'r4']\n",
      "[0.25620178453112236, 0.23248622458546903, 0.23770724063120177, 0.12545435679807823]\n",
      "['r1', 'r2', 'r3', 'r4']\n",
      "[0.10670535153856693, 0.09171723522793551, 0.022042792682386927, 0.09835242863559304]\n",
      "['r1', 'r2', 'r3', 'r4']\n"
     ]
    }
   ],
   "source": [
    "full_eval(model_specifier, modeldir, eval_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test('./models/toy-models/r1/e2', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def test2(modelpath=None, modelinput=None, device='cpu', eval_dataset=cnli_eval_datasets['chaosNLI_mnli_m']):\n",
    "\n",
    "    loader = DataLoader(dataset=eval_dataset, collate_fn=collate_fn, batch_size=1)\n",
    "    if modelpath:\n",
    "        #model = model_specifier['sequence_classification'].from_pretrained(modelpath).to(device)\n",
    "        model = BertForSequenceClassification.from_pretrained(modelpath).to(device)\n",
    "    elif modelinput:\n",
    "        model = modelinput.to(device)\n",
    "    else:\n",
    "        print('Provide a path to the model or the model itself.')\n",
    "        return None\n",
    "\n",
    "    #model = AutoModel.from_pretrained(modelpath).to(device)\n",
    "    \n",
    "\n",
    "    #modelpath = os.path.join('./models', modelname)\n",
    "    #model.load_state_dict(torch.load(modelpath, weights_only=True)) #weights_only=True\n",
    "    #model.load_state_dict(torch.load(modelname, weights_only=True)) #weights_only=True\n",
    "    #model = modelname.to(device)\n",
    "    #print(model)\n",
    "\n",
    "    y_test = []\n",
    "    y_pred = []\n",
    "    collected_logits = []\n",
    "    collected_entropies = []\n",
    "\n",
    "    #test_p = []\n",
    "    #test_h = []\n",
    "    test_id = []\n",
    "    test_log = []\n",
    "    test_oe = []\n",
    "    test_e = []\n",
    "    test_out =[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        for batch_id, batch in enumerate(tqdm.tqdm(loader)):\n",
    "            outputs = model(batch['input_ids'],\n",
    "                                attention_mask=batch['attention_masks'],\n",
    "                                token_type_ids=batch['token_type_ids'],\n",
    "                                labels=batch['label_tensors'].to(device))\n",
    "            \n",
    "            #print(len(outputs))\n",
    "            # \n",
    "            #print(batch.keys())\n",
    "            #test_p.append(batch['premise'])\n",
    "            #test_h.append(batch['hypothesis'])\n",
    "            test_id.append(batch['input_ids'])\n",
    "            \n",
    "            #outputs = outputs#.cpu()                \n",
    "\n",
    "            loss, logits = outputs[:2]\n",
    "            test_out.append(outputs)\n",
    "\n",
    "            test_log.append(logits)\n",
    "            #print(logits)\n",
    "            pred = torch.argmax(logits)\n",
    "            gold = batch['label_tensors'][0][0]#.to(device)\n",
    "            y_test.append(gold.cpu()) #cpu tensors seem to be required for some sklearn functions down the line\n",
    "            y_pred.append(pred.cpu())\n",
    "            collected_logits.append(logits)\n",
    "\n",
    "            og_ent = batch['unique_data'][0]['entropy']\n",
    "            collected_entropies.append(og_ent)\n",
    "\n",
    "            dist = torch.softmax(logits, -1).squeeze()\n",
    "            #logits.cpu().detach().numpy()\n",
    "            ent = entropy(dist.cpu().detach().numpy(), base=2)\n",
    "            #print(og_ent, ent)\n",
    "            test_oe.append(og_ent)\n",
    "            test_e.append(ent)\n",
    "            #print()\n",
    "            count+=1\n",
    "            \n",
    "            \n",
    "\n",
    "        #print(y_test, 'y_test')\n",
    "        return  test_id,test_log, test_oe, test_e, test_out #test_p , test_h\n",
    "        return y_test, y_pred, collected_logits, collected_entropies\n",
    "\n",
    "        print_metrics(y_pred, y_test, list(classes.keys()), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test_id2,test_log2, test_oe2, test_e2, test_out2 = test2(test_model, device=device, eval_dataset=cnli_eval_datasets['chaosNLI_mnli_m'])\n",
    "test_id2,test_log2, test_oe2, test_e2, test_out2 = test2(modelpath='./models2', device=device, eval_dataset=cnli_eval_datasets['chaosNLI_mnli_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_id,test_log, test_oe, test_e, test_out = test('test.pt', device=device, eval_dataset=cnli_eval_datasets['chaosNLI_mnli_m'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test, y_pred, collected_logits, collected_entropies = test('bert-base-uncased-r3-bs16-eps3-lr0.0025-e3.pt', device=device, eval_dataset=cnli_eval_datasets['chaosNLI_mnli_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_eval():\n",
    "    namelist = os.listdir('./models')\n",
    "\n",
    "    #sorts model names first by round, then by epoch:\n",
    "    sorted_namelist = sorted(namelist, key=lambda x: (x.split('-')[3], x.split('-')[7]))\n",
    "    #sorts model names first by round, then by epoch\n",
    "    for modelname in sorted_namelist:\n",
    "        \n",
    "        #modelpath = os.path.join(os.getcwd(), 'models', modelname)\n",
    "        print('Evaluating', modelname, 'on chaosNLI_mnli_m:')\n",
    "        test(modelname, device=device, eval_dataset=cnli_eval_datasets['chaosNLI_mnli_m'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted_list = sorted(list, key=lambda x: (x[0], -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "namelist = os.listdir('./models')\n",
    "sorted_namelist = sorted(namelist, key=lambda x: (x.split('-')[3], x.split('-')[7])) #sorts model names first by round, then by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting SNLI dataframe...\n",
      "Getting MNLI dataframe...\n",
      "Getting ANLI dataframes...\n",
      "Getting CNLI dataframes...\n",
      "Getting NLI variation dataframe...\n",
      "Transforming SNLI data...\n",
      "Transforming MNLI data...\n",
      "Transforming ANLI data...\n",
      "\t R1...\n",
      "\t R2...\n",
      "\t R3...\n",
      "Transforming CNLI data...\n",
      "\t chaosNLI_snli...\n",
      "\t chaosNLI_mnli_m...\n",
      "Transforming NLI variation data...\n"
     ]
    }
   ],
   "source": [
    "#uncomment for train\n",
    "\n",
    "# The transformations take a while, but I figure they're better here rather than having to do them once per epoch later\n",
    "# It gets better after SNLI and MNLI since these are quite big\n",
    "snli_train_dataset, mnli_train_dataset, anli_training_datasets, cnli_eval_datasets, NLI_var_eval_dataset = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "snli 549267\n",
    "mnli 392603\n",
    "R1 16946\n",
    "R2 45460\n",
    "R3 100459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_train(dataset, len_new_data):\n",
    "    len_kept_data = len(dataset) - len_new_data\n",
    "    new_train_set, _ = random_split(dataset, [len_kept_data, len(dataset)-len_kept_data])\n",
    "    return new_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_size = int(0.8 * len(full_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_trainsets():\n",
    "    #R1_train_data = ConcatDataset([snli_train_data, mnli_train_data])\n",
    "    \n",
    "    smnli = ConcatDataset([snli_train_dataset, mnli_train_dataset])\n",
    "\n",
    "    len_anli = 0\n",
    "    for round, dataset in anli_training_datasets.items():\n",
    "    \n",
    "        len_anli+=len(dataset)\n",
    "   \n",
    "    reduced_smnli1, _ = random_split(smnli, [len_anli, len(smnli)-len_anli])\n",
    "\n",
    "    round1 = reduced_smnli1\n",
    "\n",
    "    reduced_smnli2 = reduce_train(reduced_smnli1, len(anli_training_datasets['R1']))\n",
    "    round2 = ConcatDataset([reduced_smnli2, anli_training_datasets['R1']])\n",
    "\n",
    "    reduced_smnli3 = reduce_train(reduced_smnli2, len(anli_training_datasets['R2']))\n",
    "    round3 = ConcatDataset([reduced_smnli3, anli_training_datasets['R1'], anli_training_datasets['R2']])\n",
    "    \n",
    "    #reduced_smnli4 = reduce_train(reduced_smnli3, len(anli_training_datasets['R3']))\n",
    "    #print(len(reduced_smnli4))\n",
    "\n",
    "    round4 = ConcatDataset([anli_training_datasets['R1'], anli_training_datasets['R2'], anli_training_datasets['R3'] ])\n",
    "\n",
    "    print(len(round1), len(round2), len(round3), len(round4))\n",
    "    \n",
    "    #round2 = ConcatDataset([round1, anli_training_datasets['R1']])\n",
    "    #round3 = ConcatDataset([round2, anli_training_datasets['R2']])\n",
    "   #round4 = ConcatDataset([round3, anli_training_datasets['R3']])\n",
    "\n",
    "    return round1, round2, round3, round4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162865 162865 162865 162865\n"
     ]
    }
   ],
   "source": [
    "#uncomment for train\n",
    "r1train, r2train, r3train, r4train = prepare_trainsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(toy_run=True, overwrite=False, manual_modelname=False, log_to_file=False):\n",
    "\n",
    "    modeldir = './models/bert-base-uncased-bs32-eps6-lr5e-05'\n",
    "    \n",
    "    \n",
    "    #cnli_eval_datasets, NLI_var_eval_dataset = get_evalsets()\n",
    "    \n",
    "    if toy_run:\n",
    "        hp = toy_hyperparams\n",
    "        training_rounds = list(cnli_eval_datasets.values())\n",
    "        print('Toy run:')\n",
    "        print('Training on toy parameters')\n",
    "        print(hp)\n",
    "        print('Training on', list(cnli_eval_datasets.keys()))\n",
    "    else:\n",
    "        hp = hyperparams\n",
    "        training_rounds = (r1train, r2train, r3train, r4train)\n",
    "        print('Running full training on all four rounds. Hyperparameters:')\n",
    "        print(hp)\n",
    "\n",
    "\n",
    "    #print(hp)\n",
    "    models = {}\n",
    "\n",
    "    newpath = os.path.join(os.getcwd(), 'models')\n",
    "    if not os.path.exists(newpath):\n",
    "        os.mkdir(newpath)\n",
    "    \n",
    "    if manual_modelname:\n",
    "        dirname = manual_modelname\n",
    "    elif toy_run:\n",
    "        dirname = 'toy-models'\n",
    "    else:\n",
    "        modelname = model_specifier['model_name']\n",
    "        batch_size_str = 'bs'+str(hp['batch_size'])\n",
    "        epochs_str = 'eps'+str(hp['epochs'])\n",
    "        lr_str = 'lr'+str(hp['lr'])\n",
    "        dirname = '-'.join((modelname, batch_size_str, epochs_str, lr_str))\n",
    "    \n",
    "    dirpath = os.path.join(newpath, dirname)\n",
    "\n",
    "    if overwrite:\n",
    "        if os.path.exists(dirpath):\n",
    "            shutil.rmtree(dirpath)\n",
    "    else:\n",
    "        num = 2\n",
    "        dirpath_base =dirpath\n",
    "        while os.path.exists(dirpath):\n",
    "            dirpath = dirpath_base+'_'+str(num)\n",
    "            num+=1\n",
    "\n",
    "    os.mkdir(dirpath)\n",
    "\n",
    "    hppath = os.path.join(dirpath, 'hyperparameters.json')\n",
    "    \n",
    "    with open(hppath, 'w') as f:\n",
    "            json.dump(hp, f)\n",
    "    #os.mkdir(dirpath, 'hyperparams')\n",
    "    \n",
    "\n",
    "    \n",
    "    if torch.cuda.is_available():          \n",
    "        for round_num, round_data in enumerate(training_rounds):\n",
    "            roundnum_str = 'r'+str(round_num+1)\n",
    "            roundpath = os.path.join(dirpath, roundnum_str)\n",
    "            os.mkdir(roundpath)\n",
    "            resultspath = os.path.join(roundpath, 'results')\n",
    "            os.mkdir(resultspath)\n",
    "                #dirname = '-'.join((roundnum_str, modelname, batch_size_str, epochs_str, lr_str))\n",
    "\n",
    "            #dirpath = os.path.join(newpath, dirname)\n",
    "            #os.mkdir(dirpath)\n",
    "            print()\n",
    "            print('-------------------------------')\n",
    "            print('Training', roundnum_str)\n",
    "            round_models = train(model_specifier, round_data, roundpath, hp, logfile=log_to_file)\n",
    "            print()\n",
    "            models[roundnum_str] = round_models\n",
    "    else:\n",
    "        print('cuda unavailable')\n",
    "        return None\n",
    "        #sys.exit()\n",
    "    \n",
    "    return models\n",
    "        \n",
    "        #train(model_specifier, round, filename)\n",
    "\n",
    "    #train(model_specifier, train_ds, model_filename):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = model_specifier['model_name']\n",
    "batch_size_str = 'bs'+str(hyperparams['batch_size'])\n",
    "epochs_str = 'eps'+str(hyperparams['epochs'])\n",
    "lr_str = 'lr'+str(hyperparams['lr'])\n",
    "dirname = '-'.join((modelname, batch_size_str, epochs_str, lr_str))\n",
    "dirname = dirname+'_2'\n",
    "roundnum_str = 'r4'\n",
    "roundpath = os.path.join(dirname, roundnum_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased-bs32-eps6-lr5e-05_2/r4'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roundpath = os.path.join(os.getcwd(), 'models', roundpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [44:02<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.6154814206371139\n",
      "\n",
      "Evaluating epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:16<00:00, 94.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5222013758599124\n",
      "\n",
      "f-score for label 'entailment' is 0.5456026058631922\n",
      "f-score for label 'neutral' is 0.5634629493763756\n",
      "f-score for label 'contradiction' is 0.38220757825370677\n",
      "macro f-score is 0.49709104449775826\n",
      "\n",
      "logits are:\n",
      "tensor([[ 0.7211, -0.6332, -0.2054]], device='cuda:0')\n",
      "tensor([[ 0.5344,  0.0083, -1.1166]], device='cuda:0')\n",
      "tensor([[-0.6575,  0.9722, -0.9504]], device='cuda:0')\n",
      "tensor([[ 0.0104,  0.5398, -0.8381]], device='cuda:0')\n",
      "------------------------------------------------\n",
      "Epoch 2:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [44:35<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.3896588913100993\n",
      "\n",
      "Evaluating epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:16<00:00, 94.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5422138836772983\n",
      "\n",
      "f-score for label 'entailment' is 0.5980861244019139\n",
      "f-score for label 'neutral' is 0.5529676934635612\n",
      "f-score for label 'contradiction' is 0.40456769983686786\n",
      "macro f-score is 0.5185405059007809\n",
      "\n",
      "logits are:\n",
      "tensor([[ 0.7210, -0.2840, -0.8494]], device='cuda:0')\n",
      "tensor([[ 2.2026, -1.1779, -1.4370]], device='cuda:0')\n",
      "tensor([[ 1.4133, -0.4116, -1.3232]], device='cuda:0')\n",
      "tensor([[-0.8649,  1.9976, -1.7745]], device='cuda:0')\n",
      "------------------------------------------------\n",
      "Epoch 3:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [44:28<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.25439449695903216\n",
      "\n",
      "Evaluating epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:16<00:00, 94.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.49280800500312694\n",
      "\n",
      "f-score for label 'entailment' is 0.4737327188940092\n",
      "f-score for label 'neutral' is 0.5572354211663066\n",
      "f-score for label 'contradiction' is 0.39779005524861877\n",
      "macro f-score is 0.47625273176964483\n",
      "\n",
      "logits are:\n",
      "tensor([[-1.5769, -0.0840,  1.8061]], device='cuda:0')\n",
      "tensor([[ 0.5345, -0.9842,  0.3569]], device='cuda:0')\n",
      "tensor([[-2.1313,  2.2738, -0.6427]], device='cuda:0')\n",
      "tensor([[-1.9879,  2.5472, -1.1147]], device='cuda:0')\n",
      "------------------------------------------------\n",
      "Epoch 4:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [44:38<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.18102171369691056\n",
      "\n",
      "Evaluating epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:16<00:00, 94.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5303314571607255\n",
      "\n",
      "f-score for label 'entailment' is 0.5941358024691358\n",
      "f-score for label 'neutral' is 0.5343018563357547\n",
      "f-score for label 'contradiction' is 0.39819004524886875\n",
      "macro f-score is 0.508875901351253\n",
      "\n",
      "logits are:\n",
      "tensor([[ 0.1934, -0.0716, -0.6924]], device='cuda:0')\n",
      "tensor([[ 0.5043, -1.0086,  0.1268]], device='cuda:0')\n",
      "tensor([[-1.0822,  1.3658, -0.7335]], device='cuda:0')\n",
      "tensor([[-0.6925,  1.8984, -1.9054]], device='cuda:0')\n",
      "------------------------------------------------\n",
      "Epoch 5:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [44:36<00:00,  1.90it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.1363214034188268\n",
      "\n",
      "Evaluating epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:16<00:00, 94.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.4978111319574734\n",
      "\n",
      "f-score for label 'entailment' is 0.5178268251273345\n",
      "f-score for label 'neutral' is 0.5291338582677165\n",
      "f-score for label 'contradiction' is 0.41333333333333333\n",
      "macro f-score is 0.4867646722427948\n",
      "\n",
      "logits are:\n",
      "tensor([[-1.9468,  1.9830, -0.4993]], device='cuda:0')\n",
      "tensor([[ 2.1056, -1.1216, -1.4655]], device='cuda:0')\n",
      "tensor([[-2.2670,  0.9638,  1.1806]], device='cuda:0')\n",
      "tensor([[-1.4583,  1.5647, -0.5525]], device='cuda:0')\n",
      "------------------------------------------------\n",
      "Epoch 6:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [44:27<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.11289937087048343\n",
      "\n",
      "Evaluating epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:16<00:00, 94.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5240775484677924\n",
      "\n",
      "f-score for label 'entailment' is 0.5791190864600326\n",
      "f-score for label 'neutral' is 0.5358306188925082\n",
      "f-score for label 'contradiction' is 0.41397849462365593\n",
      "macro f-score is 0.5096427333253989\n",
      "\n",
      "logits are:\n",
      "tensor([[-0.2502,  1.4599, -1.7514]], device='cuda:0')\n",
      "tensor([[ 3.4691, -1.4240, -2.5073]], device='cuda:0')\n",
      "tensor([[-2.5682,  1.1918,  1.1003]], device='cuda:0')\n",
      "tensor([[-1.6757,  3.0545, -1.7923]], device='cuda:0')\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "round4_models = train(model_specifier, r4train, roundpath, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'dict'>\n",
      "cnli entropy, base2: 1.209800338660482\n",
      "\n",
      "numpy:\n",
      "np log2 entropy from dist: 1.2098003386604825\n",
      "np log2 entropy from count: -543.4055851114242\n",
      "\n",
      "pt, default=natlog:\n",
      "pt ent from dist: tensor(0.8386)\n",
      "pt ent from count: tensor(0.8386)\n",
      "\n",
      "scipy, default=natlog\n",
      "scipy entropy from dist: 0.8385696937829805\n",
      "scipy entropy from count: 0.8385696937829805\n",
      "scipy entropy from dist base2: 1.2098003386604825\n",
      "\n",
      "logits tensor([ 0.7211, -0.6332, -0.2054], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "probs tensor([0.6046, 0.1561, 0.2394], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "np_probs [0.60456604 0.15605876 0.23937519]\n",
      "pt entropy from logits: tensor(1.0656)\n",
      "pt entropy from softmax: tensor(0.8386)\n",
      "scipy entropy from logits: -inf\n",
      "scipy entropy from softmax: 0.9363674\n",
      "scipy entropy from logits, b2: -inf\n",
      "scipy entropy from softmax, b2: 1.3508925936675558\n",
      "scipy entropy from softmax2: 1.3508925936675558\n",
      "\n",
      "model numpy entropy, natlog 0.9363674\n",
      "model numpy entropy, b2 1.3508925\n"
     ]
    }
   ],
   "source": [
    "ent_test_model = os.path.join('./models/bert-base-uncased-bs32-eps6-lr5e-05', 'r4', 'e1')\n",
    "#os.listdir(ent_test_model)\n",
    "entropy_testing(cnli_eval_datasets['chaosNLI_mnli_m'], ent_test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running full training on all four rounds. Hyperparameters:\n",
      "{'batch_size': 32, 'epochs': 6, 'lr': 5e-05}\n",
      "\n",
      "-------------------------------\n",
      "Training r1\n",
      "Epoch 1:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [21:38<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.5560356694173016\n",
      "\n",
      "Evaluating epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 93.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5003126954346466\n",
      "\n",
      "f-score for label 'entailment' is 0.5718849840255591\n",
      "f-score for label 'neutral' is 0.4664371772805508\n",
      "f-score for label 'contradiction' is 0.4362244897959184\n",
      "macro f-score is 0.49151555036734274\n",
      "\n",
      "logits are:\n",
      "tensor([[-0.2055, -1.6975,  1.3530]], device='cuda:2')\n",
      "tensor([[ 0.9640,  0.6165, -1.3053]], device='cuda:2')\n",
      "tensor([[-0.7779,  0.3319,  0.1786]], device='cuda:2')\n",
      "tensor([[ 1.8448, -0.8016, -0.9436]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 2:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [21:43<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.3594666064934611\n",
      "\n",
      "Evaluating epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 92.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5184490306441526\n",
      "\n",
      "f-score for label 'entailment' is 0.5847389558232932\n",
      "f-score for label 'neutral' is 0.4975206611570248\n",
      "f-score for label 'contradiction' is 0.4414535666218035\n",
      "macro f-score is 0.5079043945340406\n",
      "\n",
      "logits are:\n",
      "tensor([[ 1.3271, -0.6975, -0.0983]], device='cuda:2')\n",
      "tensor([[ 0.0468,  1.0733, -0.7880]], device='cuda:2')\n",
      "tensor([[-2.2526,  0.0821,  1.9443]], device='cuda:2')\n",
      "tensor([[ 1.3821,  1.0656, -2.2963]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 3:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [21:40<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.23954731916280533\n",
      "\n",
      "Evaluating epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 93.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5053158223889931\n",
      "\n",
      "f-score for label 'entailment' is 0.5471698113207547\n",
      "f-score for label 'neutral' is 0.4991869918699187\n",
      "f-score for label 'contradiction' is 0.4538653366583541\n",
      "macro f-score is 0.5000740466163425\n",
      "\n",
      "logits are:\n",
      "tensor([[ 0.3134, -1.8172,  1.6553]], device='cuda:2')\n",
      "tensor([[ 3.0834,  0.4560, -2.8963]], device='cuda:2')\n",
      "tensor([[-2.1469,  1.0498,  0.7062]], device='cuda:2')\n",
      "tensor([[ 1.0628,  1.1924, -2.1592]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 4:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [21:40<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.1690204484490696\n",
      "\n",
      "Evaluating epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 93.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5265791119449656\n",
      "\n",
      "f-score for label 'entailment' is 0.5899053627760252\n",
      "f-score for label 'neutral' is 0.5121759622937941\n",
      "f-score for label 'contradiction' is 0.4322678843226788\n",
      "macro f-score is 0.511449736464166\n",
      "\n",
      "logits are:\n",
      "tensor([[ 2.0973, -1.0321, -0.8201]], device='cuda:2')\n",
      "tensor([[ 3.1704,  0.7837, -3.2546]], device='cuda:2')\n",
      "tensor([[ 0.0269,  1.5999, -1.7531]], device='cuda:2')\n",
      "tensor([[ 2.4923, -0.6417, -1.7425]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 5:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [21:41<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.1338824885995228\n",
      "\n",
      "Evaluating epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 93.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5015634771732332\n",
      "\n",
      "f-score for label 'entailment' is 0.5402201524132092\n",
      "f-score for label 'neutral' is 0.5068078668683812\n",
      "f-score for label 'contradiction' is 0.42589928057553955\n",
      "macro f-score is 0.4909757666190433\n",
      "\n",
      "logits are:\n",
      "tensor([[ 2.6929, -0.3646, -1.9384]], device='cuda:2')\n",
      "tensor([[ 2.0625,  2.7482, -4.3763]], device='cuda:2')\n",
      "tensor([[ 1.5546,  0.8374, -2.1716]], device='cuda:2')\n",
      "tensor([[ 3.8639, -0.1744, -3.2066]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 6:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [21:44<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.1090744101629942\n",
      "\n",
      "Evaluating epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 93.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5272045028142589\n",
      "\n",
      "f-score for label 'entailment' is 0.5660377358490566\n",
      "f-score for label 'neutral' is 0.5351681957186545\n",
      "f-score for label 'contradiction' is 0.44113263785394935\n",
      "macro f-score is 0.5141128564738868\n",
      "\n",
      "logits are:\n",
      "tensor([[ 4.0918, -1.6033, -2.1241]], device='cuda:2')\n",
      "tensor([[ 2.1181,  3.1343, -4.6836]], device='cuda:2')\n",
      "tensor([[-2.1798,  4.1833, -1.9967]], device='cuda:2')\n",
      "tensor([[ 4.8251, -0.2956, -3.8933]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "Training r2\n",
      "Epoch 1:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [28:07<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.5576156075319278\n",
      "\n",
      "Evaluating epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 93.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5215759849906192\n",
      "\n",
      "f-score for label 'entailment' is 0.5686433793663688\n",
      "f-score for label 'neutral' is 0.5235602094240838\n",
      "f-score for label 'contradiction' is 0.4253968253968254\n",
      "macro f-score is 0.5058668047290926\n",
      "\n",
      "logits are:\n",
      "tensor([[-1.6177, -1.0102,  1.7078]], device='cuda:2')\n",
      "tensor([[ 0.4187,  1.2612, -1.5426]], device='cuda:2')\n",
      "tensor([[-0.7209,  1.2764, -0.4694]], device='cuda:2')\n",
      "tensor([[ 1.3959,  0.4911, -1.7349]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 2:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [28:09<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.362235447193527\n",
      "\n",
      "Evaluating epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 92.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.47842401500938087\n",
      "\n",
      "f-score for label 'entailment' is 0.4708029197080292\n",
      "f-score for label 'neutral' is 0.5077574047954866\n",
      "f-score for label 'contradiction' is 0.4298245614035088\n",
      "macro f-score is 0.4694616286356748\n",
      "\n",
      "logits are:\n",
      "tensor([[ 1.0562, -0.0833, -1.2052]], device='cuda:2')\n",
      "tensor([[ 0.5872,  2.0281, -2.3055]], device='cuda:2')\n",
      "tensor([[-0.7986,  1.7133, -0.7606]], device='cuda:2')\n",
      "tensor([[ 1.7829,  0.8831, -2.5442]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 3:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [28:09<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.2407900101624151\n",
      "\n",
      "Evaluating epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 92.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5434646654158849\n",
      "\n",
      "f-score for label 'entailment' is 0.6227272727272727\n",
      "f-score for label 'neutral' is 0.5088161209068011\n",
      "f-score for label 'contradiction' is 0.4512372634643377\n",
      "macro f-score is 0.5275935523661371\n",
      "\n",
      "logits are:\n",
      "tensor([[-0.9515, -1.7944,  1.9320]], device='cuda:2')\n",
      "tensor([[ 0.5372,  1.3751, -1.7461]], device='cuda:2')\n",
      "tensor([[-0.2931,  0.8351, -0.4507]], device='cuda:2')\n",
      "tensor([[ 3.3980,  0.1738, -3.3402]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 4:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [28:08<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.17088851366944113\n",
      "\n",
      "Evaluating epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 93.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5390869293308318\n",
      "\n",
      "f-score for label 'entailment' is 0.583596214511041\n",
      "f-score for label 'neutral' is 0.528152260111023\n",
      "f-score for label 'contradiction' is 0.47533632286995514\n",
      "macro f-score is 0.5290282658306731\n",
      "\n",
      "logits are:\n",
      "tensor([[ 0.2974, -1.4995,  0.7891]], device='cuda:2')\n",
      "tensor([[-1.6089,  3.8365, -1.8890]], device='cuda:2')\n",
      "tensor([[-2.2810,  1.6916,  1.0477]], device='cuda:2')\n",
      "tensor([[ 2.9057,  0.5594, -3.0800]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 5:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [28:09<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.13319093215266473\n",
      "\n",
      "Evaluating epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 92.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.4878048780487805\n",
      "\n",
      "f-score for label 'entailment' is 0.5069444444444444\n",
      "f-score for label 'neutral' is 0.4954268292682927\n",
      "f-score for label 'contradiction' is 0.444141689373297\n",
      "macro f-score is 0.48217098769534467\n",
      "\n",
      "logits are:\n",
      "tensor([[ 1.2674, -2.2717,  0.5232]], device='cuda:2')\n",
      "tensor([[-1.9146,  3.9203, -1.6866]], device='cuda:2')\n",
      "tensor([[-1.3942,  2.6207, -0.9765]], device='cuda:2')\n",
      "tensor([[ 3.3543,  0.4080, -3.6303]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 6:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [28:06<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.11006774310254051\n",
      "\n",
      "Evaluating epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 92.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.4965603502188868\n",
      "\n",
      "f-score for label 'entailment' is 0.5276595744680851\n",
      "f-score for label 'neutral' is 0.5019425019425019\n",
      "f-score for label 'contradiction' is 0.4375\n",
      "macro f-score is 0.48903402547019564\n",
      "\n",
      "logits are:\n",
      "tensor([[ 3.0981, -2.0594, -1.3462]], device='cuda:2')\n",
      "tensor([[-1.1748,  4.8013, -3.1886]], device='cuda:2')\n",
      "tensor([[ 0.6220,  2.3307, -2.5826]], device='cuda:2')\n",
      "tensor([[ 2.9542,  2.2034, -4.5903]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "Training r3\n",
      "Epoch 1:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [30:57<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.5780789584038534\n",
      "\n",
      "Evaluating epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 93.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5303314571607255\n",
      "\n",
      "f-score for label 'entailment' is 0.6012658227848101\n",
      "f-score for label 'neutral' is 0.5067513899920572\n",
      "f-score for label 'contradiction' is 0.4414814814814815\n",
      "macro f-score is 0.5164995647527829\n",
      "\n",
      "logits are:\n",
      "tensor([[-1.5204, -0.9570,  2.5276]], device='cuda:2')\n",
      "tensor([[ 1.1911,  0.8918, -1.5472]], device='cuda:2')\n",
      "tensor([[-1.3153,  1.2601,  0.0151]], device='cuda:2')\n",
      "tensor([[ 1.4158,  1.1957, -2.2139]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 2:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [31:01<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.3770621106211528\n",
      "\n",
      "Evaluating epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 92.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5228267667292058\n",
      "\n",
      "f-score for label 'entailment' is 0.5294635004397538\n",
      "f-score for label 'neutral' is 0.5491803278688525\n",
      "f-score for label 'contradiction' is 0.4455611390284757\n",
      "macro f-score is 0.508068322445694\n",
      "\n",
      "logits are:\n",
      "tensor([[ 1.1522, -0.7137,  0.3013]], device='cuda:2')\n",
      "tensor([[ 2.1274,  0.3179, -1.6634]], device='cuda:2')\n",
      "tensor([[-1.8805,  1.8854, -0.3816]], device='cuda:2')\n",
      "tensor([[ 1.3285,  0.6132, -1.4542]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 3:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [31:01<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.2536836412136968\n",
      "\n",
      "Evaluating epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 92.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5121951219512195\n",
      "\n",
      "f-score for label 'entailment' is 0.575682382133995\n",
      "f-score for label 'neutral' is 0.4721984602224123\n",
      "f-score for label 'contradiction' is 0.47560975609756095\n",
      "macro f-score is 0.5078301994846561\n",
      "\n",
      "logits are:\n",
      "tensor([[ 2.4098, -0.4799, -0.9106]], device='cuda:2')\n",
      "tensor([[ 2.2502,  0.6995, -2.1619]], device='cuda:2')\n",
      "tensor([[-2.1144,  2.5064, -0.8519]], device='cuda:2')\n",
      "tensor([[ 2.0422,  0.7991, -2.2621]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 4:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [30:58<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.17906102435919502\n",
      "\n",
      "Evaluating epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 92.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5196998123827392\n",
      "\n",
      "f-score for label 'entailment' is 0.5725677830940988\n",
      "f-score for label 'neutral' is 0.49637389202256244\n",
      "f-score for label 'contradiction' is 0.4665718349928876\n",
      "macro f-score is 0.511837836703183\n",
      "\n",
      "logits are:\n",
      "tensor([[ 2.4189, -1.3606,  0.0259]], device='cuda:2')\n",
      "tensor([[ 2.6009,  0.9109, -2.9557]], device='cuda:2')\n",
      "tensor([[-3.1715,  2.6111,  0.1160]], device='cuda:2')\n",
      "tensor([[ 1.5842,  0.9953, -2.3431]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 5:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [30:58<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.138967253081775\n",
      "\n",
      "Evaluating epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 92.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.5109443402126329\n",
      "\n",
      "f-score for label 'entailment' is 0.5449915110356537\n",
      "f-score for label 'neutral' is 0.4976\n",
      "f-score for label 'contradiction' is 0.4805194805194805\n",
      "macro f-score is 0.5077036638517114\n",
      "\n",
      "logits are:\n",
      "tensor([[ 0.7530, -1.3052,  1.4089]], device='cuda:2')\n",
      "tensor([[ 2.3237,  0.2496, -2.0267]], device='cuda:2')\n",
      "tensor([[-3.8631,  2.4831,  0.9547]], device='cuda:2')\n",
      "tensor([[ 1.0666,  2.3845, -3.8690]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "Epoch 6:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 5090/5090 [30:57<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch: 0.11400099914413026\n",
      "\n",
      "Evaluating epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:17<00:00, 92.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.48530331457160725\n",
      "\n",
      "f-score for label 'entailment' is 0.4813126709206928\n",
      "f-score for label 'neutral' is 0.5049928673323824\n",
      "f-score for label 'contradiction' is 0.4520743919885551\n",
      "macro f-score is 0.4794599767472101\n",
      "\n",
      "logits are:\n",
      "tensor([[ 1.1456, -1.1632,  0.6915]], device='cuda:2')\n",
      "tensor([[ 3.1519,  0.0432, -2.4915]], device='cuda:2')\n",
      "tensor([[-3.7929,  3.6805, -0.7279]], device='cuda:2')\n",
      "tensor([[ 1.8064,  1.1069, -2.8002]], device='cuda:2')\n",
      "------------------------------------------------\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "Training r4\n",
      "Epoch 1:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:  90%| | 4572/5090 [35:59<04:04,  2.12it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 130.00 MiB. GPU 2 has a total capacity of 10.90 GiB of which 57.31 MiB is free. Including non-PyTorch memory, this process has 10.85 GiB memory in use. Of the allocated memory 10.23 GiB is allocated by PyTorch, and 455.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoy_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[104], line 74\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(toy_run, overwrite, manual_modelname, log_to_file)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, roundnum_str)\n\u001b[0;32m---> 74\u001b[0m round_models \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_specifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mround_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroundpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_to_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     76\u001b[0m models[roundnum_str] \u001b[38;5;241m=\u001b[39m round_models\n",
      "Cell \u001b[0;32mIn[107], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model_specifier, train_ds, dirpath, hyperparams, logfile)\u001b[0m\n\u001b[1;32m     46\u001b[0m                             \u001b[38;5;66;03m# so I don't need to do that separately\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''ipi.extend(batch['input_ids'])\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mipi.append('BATCHBREAK')\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03mli.extend(logits)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mlbs.extend(batch['label_tensors'])\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mlbs.append('BATCHBREAK')'''\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     64\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 2 has a total capacity of 10.90 GiB of which 57.31 MiB is free. Including non-PyTorch memory, this process has 10.85 GiB memory in use. Of the allocated memory 10.23 GiB is allocated by PyTorch, and 455.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "models = main(toy_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#stored = models['r1']['e3']\n",
    "loaded = model_specifier['sequence_classification'].from_pretrained('./models/testing_model_4/r1/e1').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1599/1599 [00:16<00:00, 94.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy is 0.36460287679799874\n",
      "\n",
      "f-score for label 'entailment' is 0.0\n",
      "f-score for label 'neutral' is 0.534372135655362\n",
      "f-score for label 'contradiction' is 0.0\n",
      "macro f-score is 0.178124045218454\n",
      "\n",
      "logits are:\n",
      "tensor([[-9.2412,  8.8403,  7.3992]], device='cuda:1')\n",
      "tensor([[-9.2412,  8.8403,  7.3992]], device='cuda:1')\n",
      "tensor([[-9.2412,  8.8403,  7.3992]], device='cuda:1')\n",
      "tensor([[-9.2412,  8.8403,  7.3992]], device='cuda:1')\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test(modelin=loaded, device=device, eval_dataset=cnli_eval_datasets['chaosNLI_mnli_m'], logfile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1599 [00:00<13:08,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.209800338660482 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.295225451536183 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.4041074513870861 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.11878903655513 1.3814821158806183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1599 [00:00<01:32, 17.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.048018094115863 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.217796811598595 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.071206339756579 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.104746783742209 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.054335564133742 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.219240704636849 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "0.795040279384522 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "0.7579911871785621 1.3814821158806183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/1599 [00:00<01:36, 16.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.1456781689999 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.054335564133742 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "0.746539872415499 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.044667051917939 1.3814822018719437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.209800338660482 1.3814821158806183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1599 [00:00<00:55, 28.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.295225451536183 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.4041074513870861 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.11878903655513 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.048018094115863 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.217796811598595 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.071206339756579 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.104746783742209 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.054335564133742 1.3814822018719437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1599 [00:00<00:47, 33.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.219240704636849 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "0.795040279384522 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "0.7579911871785621 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.1456781689999 1.3814821158806183\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.054335564133742 1.3814822018719437\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "0.746539872415499 1.3814822018719437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/1599 [00:00<00:53, 29.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_masks', 'unique_data', 'label_tensors'])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "1.044667051917939 1.3814822018719437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "l_test_id,l_test_log, l_test_oe, l_test_e, l_test_out, l_y_test, l_y_pred, l_collected_logits, l_collected_entropies =test(modelpath=False, modelin= loaded, device=device)\n",
    "s_test_id,s_test_log, s_test_oe, s_test_e, s_test_out, s_y_test, s_y_pred, s_collected_logits, s_collected_entropies =test(modelpath=False, modelin= stored, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n",
      "tensor([[ 0.2848, -0.1289, -1.2064]]) tensor([[ 0.2848, -0.1289, -1.2064]])\n"
     ]
    }
   ],
   "source": [
    "for l, s in zip(l_collected_logits, s_collected_logits):\n",
    "    print(l, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_encoding': {'input_ids': tensor([  101,  2054,  2024,  2027,  2725,  2157,  2008,  1045,  2106,  3308,\n",
       "           1029,  2672,  2026,  6707,  2428,  2001,  2074,  1037, 11576,  4926,\n",
       "           1012,   102,  2009,  2003,  2825,  1045,  2001,  2074,  1037, 11576,\n",
       "           4926,  1012,   102], device='cuda:0'),\n",
       "  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')},\n",
       " 'premise': 'What are they doing right that I did wrong? Maybe my mistake really was just a freak accident.',\n",
       " 'hypothesis': 'It is possible I was just a freak accident.',\n",
       " 'input_length': 33,\n",
       " 'unique_data': labels           [49.999903547520425, -49.99995111441214, -49.9...\n",
       " normed-labels    [-1.049158911374859, -2.056001753990448, -1.38...\n",
       " Name: 0, dtype: object}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLI_var_eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 4, 'epochs': 2, 'lr': 0.1}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open(\"NLI_variation_data.jsonl\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
