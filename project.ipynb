{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "#from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')\n",
    "else:\n",
    "    device = torch.device('cpu') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = requests.get(url)\n",
    "#data = f.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_url = \"https://raw.githubusercontent.com/huhailinguist/curing-SICK/refs/heads/main/SICK_cured_20221112.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sick_r = requests.get(sick_url)\n",
    "with open(\"sick_nli.csv\", \"wb\") as f:\n",
    "    f.write(sick_r.content)\n",
    "sickdf = pd.read_csv(filepath_or_buffer=\"./sick_nli.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for index, row in sickdf.iterrows(): #Logic_label,Commonsense_label\n",
    "    #print(index, row['Logic_label'], )\n",
    "    if row['Logic_label'] != row['Commonsense_label']:\n",
    "        print(index, row['Logic_label'], row['Commonsense_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anli = load_dataset('facebook/anli', split=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['ANLI_analysis_300dev_sampleA2.csv',\n",
    " 'ANLI_analysis_hard_dev.csv',\n",
    " 'ANLI_analysis_r1_dev.csv',\n",
    " 'ANLI_analysis_r2_dev.csv',\n",
    " 'ANLI_analysis_r3_dev.csv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nli_var_url = \"https://raw.githubusercontent.com/epavlick/NLI-variation-data/refs/heads/master/sentence-pair-analysis/preprocessed-data.jsonl\"\n",
    "nli_var_r = requests.get(nli_var_url)\n",
    "\n",
    "with open(\"./data/NLI_variation_data.jsonl\", \"wb\") as f:\n",
    "    f.write(nli_var_r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_datasets():\n",
    "    datasets = {}\n",
    "    walking = os.walk('./data')\n",
    "    for item in walking:\n",
    "        if not item[1]:\n",
    "            filepath = item[0]\n",
    "            filenames = item[2]\n",
    "            for filename in filenames:\n",
    "                name, extension = os.path.splitext(filename)\n",
    "                if extension == '.jsonl':\n",
    "                    df = pd.read_json(path_or_buf=os.path.join(filepath, filename), lines=True)\n",
    "                if extension == '.csv':\n",
    "                    df = pd.read_csv(filepath_or_buffer=os.path.join(filepath, filename))\n",
    "\n",
    "                subset_spec = filepath.split('data/')[1]\n",
    "                subset_spec = subset_spec.split('/')\n",
    "                if len(subset_spec) == 2:\n",
    "\n",
    "                    if subset_spec[0] not in datasets:\n",
    "                        datasets[subset_spec[0]] = {}\n",
    "                    if subset_spec[1] not in datasets[subset_spec[0]]:\n",
    "                        datasets[subset_spec[0]] = subset_spec[1]\n",
    "                    datasets[subset_spec[0]][subset_spec[1]] = \n",
    "                #print(subset_spec)\n",
    "                print()\n",
    "                    #name, extension = os.path.splitext(file)\n",
    "                    #    if extension == '.jsonl':\n",
    "                        #    datasets[]\n",
    "                            #return pd.read_json(path_or_buf=itempath, lines=True)\n",
    "                        #if extension == '.csv':\n",
    "                            #return pd.read_csv(filepath_or_buffer=itempath)\n",
    "    #skips .txt files since these are only readmes in this case\n",
    "                #os.getcwd()\n",
    "                \n",
    "            #datasets\n",
    "            #print(item.spli)\n",
    "        \n",
    "    #possible_file = load\n",
    "    #datasets['nli_var'] = pd.read_json(path_or_buf='./data/NLI_variation_data.jsonl', lines=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#from https://www.geeksforgeeks.org/python-list-all-files-in-directory-and-subdirectories/\n",
    "\n",
    "def dict_extension(subdir):\n",
    "    return subdir\n",
    "\n",
    "def create_dataframe_dict(dirpath):\n",
    "    dataframe_dict = {}\n",
    "    for entry in os.listdir(dirpath):\n",
    "        full_path = os.path.join(dirpath, entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            create_dataframe_dict(full_path)\n",
    "        else:\n",
    "            name, extension = os.path.splitext(full_path)\n",
    "            if extension == '.jsonl':\n",
    "                df = pd.read_json(path_or_buf=full_path, lines=True)\n",
    "            elif extension == '.csv':\n",
    "                df = pd.read_csv(filepath_or_buffer=full_path)\n",
    "            else:\n",
    "                continue\n",
    "            subset_spec = name.split('data/')[-1]\n",
    "            dict_branch = subset_spec.split('/') + df\n",
    "            #if subitems[0] not in dataframe_dict:\n",
    "            #    dataframe_dict[subitems[0]] = {}\n",
    "            \n",
    "            subdir_dict = {}\n",
    "            iters = 0\n",
    "            for i, item in enumerate(dict_branch):\n",
    "                if item not in dataframe_dict[]\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "            #for i in range(len(dict_branch, -1, -1)):\n",
    "            #    subdir_dict[dict_branch[i]]\n",
    "\n",
    "\n",
    "\n",
    "            print(subdirs)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    '''anli1_path = dirpath\n",
    "    for entry in os.listdir(anli1_path):\n",
    "        full_path = os.path.join(anli1_path, entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            get_anli_df(full_path)\n",
    "\n",
    "        df = pd.read_json(path_or_buf=anli1_path, lines=True)\n",
    "    name, extension = os.path.splitext(anli1_path)\n",
    "    subset_spec = name.split('data/')[-1]\n",
    "    branch = subset_spec.split('/') + df\n",
    "    print(subset_spec)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_dict(setname):\n",
    "    df_dict = {}\n",
    "    dirpath = './data/' + setname\n",
    "    walking = os.walk(dirpath)\n",
    "\n",
    "    for root, dirs, files in walking:\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            name, extension = os.path.splitext(full_path)\n",
    "            if extension == '.jsonl':\n",
    "                df = pd.read_json(path_or_buf=full_path, lines=True)\n",
    "            elif extension == '.csv':\n",
    "                df = pd.read_csv(filepath_or_buffer=full_path)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if setname == 'anli':\n",
    "                round, subset = name.split('/')[-2:]\n",
    "                if round not in df_dict:\n",
    "                    df_dict[round] = {}\n",
    "                if subset not in df_dict[round]:\n",
    "                    df_dict[round][subset] = df\n",
    "            else:\n",
    "                subset = name.split('/')[-1]\n",
    "                df_dict[subset] = df\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anli1 = get_df_dict('anli')\n",
    "anli2 = get_df_dict('anli_reanalyzed')\n",
    "cnli = get_df_dict('chaosNLI_v1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>context</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "      <th>model_label</th>\n",
       "      <th>emturk</th>\n",
       "      <th>genre</th>\n",
       "      <th>reason</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bf628cf5-6948-4595-a6d0-daf99beb47b9</td>\n",
       "      <td>Linguistics is the scientific study of languag...</td>\n",
       "      <td>Form and meaning are the only aspects of langu...</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>Linguistics involves an analysis of language f...</td>\n",
       "      <td>r1_dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e66af435-eef1-491b-af2a-4825d54611e1</td>\n",
       "      <td>Franco Zeffirelli, KBE Grande Ufficiale OMRI (...</td>\n",
       "      <td>Franco Zeffirelli had a political career</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>Franco Zeffirelli was a senator so he had a po...</td>\n",
       "      <td>r1_dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51941f10-59f7-420a-b260-38d10f225c9c</td>\n",
       "      <td>Eme 15 is the self-titled debut studio album b...</td>\n",
       "      <td>Eme 15 was released in Mexico and Latin Americ...</td>\n",
       "      <td>c</td>\n",
       "      <td>e</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>The album was released in June 26, 2012 not Ju...</td>\n",
       "      <td>r1_dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73520f08-c78c-4c98-bf72-2df71b1e83d2</td>\n",
       "      <td>Almost Sunrise is a 2016 American documentary ...</td>\n",
       "      <td>Tom and Anthony have both killed someone.</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>True</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>The prompt references combat experience, but i...</td>\n",
       "      <td>r1_dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28b13fa5-6291-4391-b5bb-83c43f1f1fe6</td>\n",
       "      <td>Sergei Mikhailovich Grinkov (Russian: Серге́й ...</td>\n",
       "      <td>Sergei Mikhailovich Grinkov became the 1988 Ol...</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>It has been stated that Sergei Mikhailovich Gr...</td>\n",
       "      <td>r1_dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0b445f59-f872-487f-9cf0-07a1fc681986</td>\n",
       "      <td>\"Girl of My Dreams\" is a song by Canadian pop ...</td>\n",
       "      <td>Canadian band The Moffatts continued their rac...</td>\n",
       "      <td>n</td>\n",
       "      <td>e</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>It's neither because it's not specificed if th...</td>\n",
       "      <td>r1_dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>cafd7711-da43-4eb8-b9d9-ec8cf118d944</td>\n",
       "      <td>The Green Mile is a 1996 serial novel written ...</td>\n",
       "      <td>John Coffey was on death row and was killed in...</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>True</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>It doesn't say whether he was killed or not, o...</td>\n",
       "      <td>r1_dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>fb601ec1-4e6a-4528-9c69-2208a7ac0bca</td>\n",
       "      <td>Nowa Słupia is a village in Kielce County, Świ...</td>\n",
       "      <td>There is a Greater Poland.</td>\n",
       "      <td>n</td>\n",
       "      <td>c</td>\n",
       "      <td>True</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>The prompt referenced a \"Lesser Poland\". Witho...</td>\n",
       "      <td>r1_dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>f7627eda-6743-47f7-b91e-abe36be17103</td>\n",
       "      <td>Kuch... Diiil Se is a Hindi language Indian te...</td>\n",
       "      <td>Diiil Se is a Hindi language Indian television...</td>\n",
       "      <td>c</td>\n",
       "      <td>e</td>\n",
       "      <td>False</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>Diiil Se is a Hindi language Indian television...</td>\n",
       "      <td>r1_dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>4a535a7c-b5e5-4722-8376-e7355cca1ead</td>\n",
       "      <td>Richmond Hill (2016 population 195,022) is a t...</td>\n",
       "      <td>The York Region town of Richmond Hill, the tow...</td>\n",
       "      <td>n</td>\n",
       "      <td>e</td>\n",
       "      <td>True</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>The computer thinks it is correct because I us...</td>\n",
       "      <td>r1_dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      uid  \\\n",
       "0    bf628cf5-6948-4595-a6d0-daf99beb47b9   \n",
       "1    e66af435-eef1-491b-af2a-4825d54611e1   \n",
       "2    51941f10-59f7-420a-b260-38d10f225c9c   \n",
       "3    73520f08-c78c-4c98-bf72-2df71b1e83d2   \n",
       "4    28b13fa5-6291-4391-b5bb-83c43f1f1fe6   \n",
       "..                                    ...   \n",
       "995  0b445f59-f872-487f-9cf0-07a1fc681986   \n",
       "996  cafd7711-da43-4eb8-b9d9-ec8cf118d944   \n",
       "997  fb601ec1-4e6a-4528-9c69-2208a7ac0bca   \n",
       "998  f7627eda-6743-47f7-b91e-abe36be17103   \n",
       "999  4a535a7c-b5e5-4722-8376-e7355cca1ead   \n",
       "\n",
       "                                               context  \\\n",
       "0    Linguistics is the scientific study of languag...   \n",
       "1    Franco Zeffirelli, KBE Grande Ufficiale OMRI (...   \n",
       "2    Eme 15 is the self-titled debut studio album b...   \n",
       "3    Almost Sunrise is a 2016 American documentary ...   \n",
       "4    Sergei Mikhailovich Grinkov (Russian: Серге́й ...   \n",
       "..                                                 ...   \n",
       "995  \"Girl of My Dreams\" is a song by Canadian pop ...   \n",
       "996  The Green Mile is a 1996 serial novel written ...   \n",
       "997  Nowa Słupia is a village in Kielce County, Świ...   \n",
       "998  Kuch... Diiil Se is a Hindi language Indian te...   \n",
       "999  Richmond Hill (2016 population 195,022) is a t...   \n",
       "\n",
       "                                            hypothesis label model_label  \\\n",
       "0    Form and meaning are the only aspects of langu...     c           n   \n",
       "1             Franco Zeffirelli had a political career     e           n   \n",
       "2    Eme 15 was released in Mexico and Latin Americ...     c           e   \n",
       "3            Tom and Anthony have both killed someone.     n           c   \n",
       "4    Sergei Mikhailovich Grinkov became the 1988 Ol...     c           n   \n",
       "..                                                 ...   ...         ...   \n",
       "995  Canadian band The Moffatts continued their rac...     n           e   \n",
       "996  John Coffey was on death row and was killed in...     n           c   \n",
       "997                         There is a Greater Poland.     n           c   \n",
       "998  Diiil Se is a Hindi language Indian television...     c           e   \n",
       "999  The York Region town of Richmond Hill, the tow...     n           e   \n",
       "\n",
       "     emturk      genre                                             reason  \\\n",
       "0     False  wikipedia  Linguistics involves an analysis of language f...   \n",
       "1     False  wikipedia  Franco Zeffirelli was a senator so he had a po...   \n",
       "2     False  wikipedia  The album was released in June 26, 2012 not Ju...   \n",
       "3      True  wikipedia  The prompt references combat experience, but i...   \n",
       "4     False  wikipedia  It has been stated that Sergei Mikhailovich Gr...   \n",
       "..      ...        ...                                                ...   \n",
       "995   False  wikipedia  It's neither because it's not specificed if th...   \n",
       "996    True  wikipedia  It doesn't say whether he was killed or not, o...   \n",
       "997    True  wikipedia  The prompt referenced a \"Lesser Poland\". Witho...   \n",
       "998   False  wikipedia  Diiil Se is a Hindi language Indian television...   \n",
       "999    True  wikipedia  The computer thinks it is correct because I us...   \n",
       "\n",
       "        tag  \n",
       "0    r1_dev  \n",
       "1    r1_dev  \n",
       "2    r1_dev  \n",
       "3    r1_dev  \n",
       "4    r1_dev  \n",
       "..      ...  \n",
       "995  r1_dev  \n",
       "996  r1_dev  \n",
       "997  r1_dev  \n",
       "998  r1_dev  \n",
       "999  r1_dev  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anli1['R1']['dev']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anli2 = {}\n",
    "anli2_path = './data/anli_annot_v0.2'\n",
    "anli2_300_dev = pd.read_csv(filepath_or_buffer=os.path.join(anli2_path, 'ANLI_analysis_300dev_sampleA2.csv'))\n",
    "anli2_hard_dev = pd.read_csv(filepath_or_buffer=os.path.join(anli2_path, 'ANLI_analysis_hard_dev.csv'))\n",
    "anli2_r1_dev = pd.read_csv(filepath_or_buffer=os.path.join(anli2_path, 'ANLI_analysis_r1_dev.csv'))\n",
    "anli2_r2_dev = pd.read_csv(filepath_or_buffer=os.path.join(anli2_path, 'ANLI_analysis_r2_dev.csv'))\n",
    "anli2_r3_dev = pd.read_csv(filepath_or_buffer=os.path.join(anli2_path, 'ANLI_analysis_r3_dev.csv'))\n",
    "\n",
    "cnli_mnli = pd.read_json(path_or_buf=os.path.join(chaosnli_path, 'chaosNLI_mnli_m.jsonl'), lines=True)\n",
    "cnli_mnli = pd.read_json(path_or_buf=os.path.join(chaosnli_path, 'chaosNLI_snli.jsonl'), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_var = pd.read_json(path_or_buf='./data/NLI_variation/NLI_variation_data.jsonl', lines=True)\n",
    "\n",
    "anli1 = get_df_dict('anli')\n",
    "anli2 = get_df_dict('anli_reanalyzed')\n",
    "cnli = get_df_dict('chaosNLI_v1.0')\n",
    "\n",
    "#os.listdir(anli2_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'context', 'hypothesis', 'label', 'model_label', 'emturk',\n",
       "       'genre', 'reason', 'tag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anli1['R1']['dev'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['context', 'statement', 'reason', 'adverary_pred', 'gold_label', 'uid',\n",
      "       'cid', 'eid', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'A2Code',\n",
      "       'A2Notes'],\n",
      "      dtype='object')\n",
      "Index(['context', 'statement', 'reason', 'adversary_pred', 'gold_label',\n",
      "       'A1Code', 'A1Notes', 'eid', 'uid', 'round'],\n",
      "      dtype='object')\n",
      "Index(['context', 'statement', 'reason', 'adversary_pred', 'gold_label',\n",
      "       'A1Code', 'A1Notes', 'eid', 'uid', 'tags', 'preds-roberta-from-r2',\n",
      "       'preds-roberta-from-r2PredLabel', 'preds-roberta-from-r3',\n",
      "       'preds-roberta-from-r3PredLabel', 'preds-bert-r1',\n",
      "       'preds-bert-r1PredLabel', 'preds-anli-roberta-large',\n",
      "       'preds-anli-roberta-largePredLabel', 'preds-anli-bart-large',\n",
      "       'preds-anli-bart-largePredLabel', 'preds-anli-xlnet-large',\n",
      "       'preds-anli-xlnet-largePredLabel', 'preds-anli-electra-large',\n",
      "       'preds-anli-electra-largePredLabel', 'preds-anli-albert-xxlarge',\n",
      "       'preds-anli-albert-xxlargePredLabel'],\n",
      "      dtype='object')\n",
      "Index(['context', 'eid', 'statement', 'reason', 'adversary_pred', 'gold_label',\n",
      "       'uid', 'A1Code', 'A1Notes', 'tags', 'preds-roberta-from-r2',\n",
      "       'preds-roberta-from-r2PredLabel', 'preds-roberta-from-r3',\n",
      "       'preds-roberta-from-r3PredLabel', 'preds-bert-r1',\n",
      "       'preds-bert-r1PredLabel', 'preds-anli-roberta-large',\n",
      "       'preds-anli-roberta-largePredLabel', 'preds-anli-bart-large',\n",
      "       'preds-anli-bart-largePredLabel', 'preds-anli-xlnet-large',\n",
      "       'preds-anli-xlnet-largePredLabel', 'preds-anli-electra-large',\n",
      "       'preds-anli-electra-largePredLabel', 'preds-anli-albert-xxlarge',\n",
      "       'preds-anli-albert-xxlargePredLabel'],\n",
      "      dtype='object')\n",
      "Index(['context', 'statement', 'reason', 'adversary_pred', 'gold_label',\n",
      "       'A1Code', 'A1Notes', 'eid', 'tags', 'uid', 'preds-roberta-from-r2',\n",
      "       'preds-roberta-from-r2PredLabel', 'preds-roberta-from-r3',\n",
      "       'preds-roberta-from-r3PredLabel', 'preds-bert-r1',\n",
      "       'preds-bert-r1PredLabel', 'preds-anli-roberta-large',\n",
      "       'preds-anli-roberta-largePredLabel', 'preds-anli-bart-large',\n",
      "       'preds-anli-bart-largePredLabel', 'preds-anli-xlnet-large',\n",
      "       'preds-anli-xlnet-largePredLabel', 'preds-anli-electra-large',\n",
      "       'preds-anli-electra-largePredLabel', 'preds-anli-albert-xxlarge',\n",
      "       'preds-anli-albert-xxlargePredLabel'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for item in anli2.values():\n",
    "    print(item.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'label_counter', 'majority_label', 'label_dist', 'label_count',\n",
      "       'entropy', 'example', 'old_label'],\n",
      "      dtype='object')\n",
      "Index(['uid', 'label_counter', 'majority_label', 'label_dist', 'label_count',\n",
      "       'entropy', 'example', 'old_label', 'old_labels'],\n",
      "      dtype='object')\n",
      "Index(['uid', 'label_counter', 'majority_label', 'label_dist', 'label_count',\n",
      "       'entropy', 'example', 'old_label', 'old_labels'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for item in cnli.values():\n",
    "    print(item.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['premise', 'hypothesis', 'task', 'original-dataset-label', 'id',\n",
       "       'labels', 'normed-labels', 'num-NA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nli_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIData(Dataset):\n",
    "    def __init__(self, subset, classes, lang):\n",
    "\n",
    "        '''\n",
    "        Subclass of pytorch Dataset\n",
    "\n",
    "        args:\n",
    "\n",
    "        '''\n",
    "        data_dict = {}\n",
    "        indices = []\n",
    "        \n",
    "        for pic, label, parent_path in subset:\n",
    "         \n",
    "            data_dict[pic] = {'label': label, 'fullpath': os.path.join(parent_path, pic)}\n",
    "            indices.append(pic)\n",
    "\n",
    "        self.parent_dir = training_dir\n",
    "        self.data_dict = data_dict\n",
    "        self.indices = indices\n",
    "        self.classes = list(classes)\n",
    "        self.class_to_idx = {c: idx for idx, c in enumerate(self.classes)}\n",
    "\n",
    "        #partially from https://www.geeksforgeeks.org/converting-an-image-to-a-torch-tensor-in-python/\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float),\n",
    "            transforms.Resize((72, 48))\n",
    "            ])\n",
    "        self.device = device\n",
    "        self.lang = lang    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.indices[idx]\n",
    "\n",
    "        label_str = self.data_dict[img_name]['label']\n",
    "        label = torch.as_tensor(self.class_to_idx[label_str])\n",
    "        pil_image = Image.open(self.data_dict[img_name]['fullpath'])\n",
    "        img_tensor = self.transform(pil_image).to(self.device)\n",
    "\n",
    "        return img_tensor, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label normed-labels\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>task</th>\n",
       "      <th>original-dataset-label</th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "      <th>normed-labels</th>\n",
       "      <th>num-NA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are they doing right that I did wrong? Ma...</td>\n",
       "      <td>It is possible I was just a freak accident.</td>\n",
       "      <td>mnli</td>\n",
       "      <td>entailment</td>\n",
       "      <td>73712e</td>\n",
       "      <td>[49.999903547520425, -49.99995111441214, -49.9...</td>\n",
       "      <td>[-1.049158911374859, -2.056001753990448, -1.38...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tony bent the rod .</td>\n",
       "      <td>Tony caused the bending .</td>\n",
       "      <td>dnc</td>\n",
       "      <td>entailed</td>\n",
       "      <td>verbnet_406516</td>\n",
       "      <td>[42.000081280187395, 50.00003388861542, 50.000...</td>\n",
       "      <td>[1.800001155931179, 0.788906072327013, 1.26332...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The M-2000 uses a commercially manufactured Nb...</td>\n",
       "      <td>Maglev is commercially used.</td>\n",
       "      <td>rte2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>121</td>\n",
       "      <td>[49.99992665451104, 49.99994617658978, 50.0000...</td>\n",
       "      <td>[-0.549657144036656, 0.91449856989563, 0.71004...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was walking down the sidewalk</td>\n",
       "      <td>I went over to it and looked over the items.</td>\n",
       "      <td>joci</td>\n",
       "      <td>0</td>\n",
       "      <td>ROC-10260</td>\n",
       "      <td>[-8.93114206056929e-05, 22.999929665966544, -2...</td>\n",
       "      <td>[-0.895102571668803, -1.099478975416834, -1.23...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Something is drawing them up from all over the...</td>\n",
       "      <td>Once the shell completely cracks, they will ke...</td>\n",
       "      <td>mnli</td>\n",
       "      <td>neutral</td>\n",
       "      <td>48208n</td>\n",
       "      <td>[12.999964376296033, 4.1744154511169876e-05, -...</td>\n",
       "      <td>[-0.24547615792878702, -0.9757304693660971, -1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>The sunset colors were not sunset.</td>\n",
       "      <td>The colors looked like sunset but were actuall...</td>\n",
       "      <td>mnli</td>\n",
       "      <td>neutral</td>\n",
       "      <td>88237n</td>\n",
       "      <td>[50.00001421913623, 34.00007343862095, 50.0000...</td>\n",
       "      <td>[-0.930862967643553, 0.32046805003406703, 0.35...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Once or twice a day, Ms.</td>\n",
       "      <td>It only happens every few weeks.</td>\n",
       "      <td>mnli</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>939c</td>\n",
       "      <td>[26.00002223884001, -48.999918914899624, -50.0...</td>\n",
       "      <td>[-0.8424180612794541, -1.258304666550468, -1.8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>I have a 1986 Coca-Cola Grand Union 100 Year S...</td>\n",
       "      <td>The Statue of Liberty was built in 1986.</td>\n",
       "      <td>rte2</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>98</td>\n",
       "      <td>[-49.9999812383724, -50.0000581672619, -50.000...</td>\n",
       "      <td>[-0.59124431230086, -1.131745977920005, -0.049...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Prime Minister Mahmoud Abbas has offered 'the ...</td>\n",
       "      <td>Mahmoud Abbas has won a landslide victory in S...</td>\n",
       "      <td>rte2</td>\n",
       "      <td>entailment</td>\n",
       "      <td>494</td>\n",
       "      <td>[50.000000291077725, 49.99994247188905, 50.000...</td>\n",
       "      <td>[0.9034810056030711, 0.612514023445426, 0.9033...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>The United States , Israel and Australia have ...</td>\n",
       "      <td>Australia is a location</td>\n",
       "      <td>dnc</td>\n",
       "      <td>entailed</td>\n",
       "      <td>ner1_161162</td>\n",
       "      <td>[-0.9999954236170481, 50.000097010784515, 50.0...</td>\n",
       "      <td>[-0.8398721961244261, 1.459742768266124, 1.003...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               premise  \\\n",
       "0    What are they doing right that I did wrong? Ma...   \n",
       "1                                  Tony bent the rod .   \n",
       "2    The M-2000 uses a commercially manufactured Nb...   \n",
       "3                      I was walking down the sidewalk   \n",
       "4    Something is drawing them up from all over the...   \n",
       "..                                                 ...   \n",
       "491                 The sunset colors were not sunset.   \n",
       "492                           Once or twice a day, Ms.   \n",
       "493  I have a 1986 Coca-Cola Grand Union 100 Year S...   \n",
       "494  Prime Minister Mahmoud Abbas has offered 'the ...   \n",
       "495  The United States , Israel and Australia have ...   \n",
       "\n",
       "                                            hypothesis  task  \\\n",
       "0          It is possible I was just a freak accident.  mnli   \n",
       "1                            Tony caused the bending .   dnc   \n",
       "2                         Maglev is commercially used.  rte2   \n",
       "3         I went over to it and looked over the items.  joci   \n",
       "4    Once the shell completely cracks, they will ke...  mnli   \n",
       "..                                                 ...   ...   \n",
       "491  The colors looked like sunset but were actuall...  mnli   \n",
       "492                   It only happens every few weeks.  mnli   \n",
       "493           The Statue of Liberty was built in 1986.  rte2   \n",
       "494  Mahmoud Abbas has won a landslide victory in S...  rte2   \n",
       "495                            Australia is a location   dnc   \n",
       "\n",
       "    original-dataset-label              id  \\\n",
       "0               entailment          73712e   \n",
       "1                 entailed  verbnet_406516   \n",
       "2                  neutral             121   \n",
       "3                        0       ROC-10260   \n",
       "4                  neutral          48208n   \n",
       "..                     ...             ...   \n",
       "491                neutral          88237n   \n",
       "492          contradiction            939c   \n",
       "493          contradiction              98   \n",
       "494             entailment             494   \n",
       "495               entailed     ner1_161162   \n",
       "\n",
       "                                                labels  \\\n",
       "0    [49.999903547520425, -49.99995111441214, -49.9...   \n",
       "1    [42.000081280187395, 50.00003388861542, 50.000...   \n",
       "2    [49.99992665451104, 49.99994617658978, 50.0000...   \n",
       "3    [-8.93114206056929e-05, 22.999929665966544, -2...   \n",
       "4    [12.999964376296033, 4.1744154511169876e-05, -...   \n",
       "..                                                 ...   \n",
       "491  [50.00001421913623, 34.00007343862095, 50.0000...   \n",
       "492  [26.00002223884001, -48.999918914899624, -50.0...   \n",
       "493  [-49.9999812383724, -50.0000581672619, -50.000...   \n",
       "494  [50.000000291077725, 49.99994247188905, 50.000...   \n",
       "495  [-0.9999954236170481, 50.000097010784515, 50.0...   \n",
       "\n",
       "                                         normed-labels  num-NA  \n",
       "0    [-1.049158911374859, -2.056001753990448, -1.38...       0  \n",
       "1    [1.800001155931179, 0.788906072327013, 1.26332...       0  \n",
       "2    [-0.549657144036656, 0.91449856989563, 0.71004...       0  \n",
       "3    [-0.895102571668803, -1.099478975416834, -1.23...       0  \n",
       "4    [-0.24547615792878702, -0.9757304693660971, -1...       0  \n",
       "..                                                 ...     ...  \n",
       "491  [-0.930862967643553, 0.32046805003406703, 0.35...       0  \n",
       "492  [-0.8424180612794541, -1.258304666550468, -1.8...       0  \n",
       "493  [-0.59124431230086, -1.131745977920005, -0.049...       0  \n",
       "494  [0.9034810056030711, 0.612514023445426, 0.9033...       0  \n",
       "495  [-0.8398721961244261, 1.459742768266124, 1.003...       0  \n",
       "\n",
       "[496 rows x 8 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nliv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NLIv['normed-labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLIv['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NLI_variation_data.jsonl\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
